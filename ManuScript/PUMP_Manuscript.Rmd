---
title: "PUMP manuscript draft"
output:
  pdf_document: default
  html_document:
    df_print: paged
  html_notebook: default
bibliography: refs.bib
geometry: "left=3cm,right=3cm,top=2cm,bottom=2cm"
header-includes:
    - \usepackage{setspace}\doublespacing
---
Power Under Multiplicity Project (PUMP): An R package for estimating statistical power, minimum detectable effect sizes (MDES's) and sample sizes when adjusting for multiple hypothesis tests
Draft outline

```{r setup, include = FALSE}
library( kableExtra )
library( knitr )
library( pum )
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 4,
  fig.align = "center"
)
options(knitr.kable.NA = '')
library( tidyverse )
library( knitr )
theme_set( theme_minimal() )

```

# TODO list

(written by Kristen)

Topics where I think we may need more detail:

- Luke: would you like to add a paragraph on the optimization procedure?
- give more context for people who aren't familiar with multilevel experiments. What is blocking? clustering? Why might you want to choose a particular RCT design?
- effect size: what it is, how to decide what it is
-    KP: I put some minimal explanation about this at the very start of the paper and added a reference. We can discuss how much more detail. 
- correlation between test statistics, relationship to correlation between outcomes
-    KP: There is a section titled "understanding design parameters" - let's discuss how to fill out this section - which parameters to discuss and level of detail. Perhaps this is where this discussion of correlation of test statistics might go? I noted in Luke's vignette, he states: For further discussion of selecting these parameters see, for example, CITE Porter original paper. So let's figure out strategy for what to put in this paper and what to refer to. 
- explanation of all MTP procedures, like in original paper
-   KP: Maybe this can be an appendix? What did you note about appendices in the journal? 

Optional topics:

- advice on binary outcomes, consider how it affects assumptions
- reference to literature on summarizing/collapsing outcomes?
-   KP: This seems beyond scope. Luke makes a nice comment (footnote) that we are agnostic on this issue and refers to literature. We could possibly move this up too. 


# Overview

A frequent task for researchers designing randomized control trials (RCTs) is to determine the statistical power of a study - the probability of detecting an effect of a particular size or larger, given that the effect truly exists. 
This task may be done prospectively in order to guide design decisions.
In this case, researchers make assumptions about data paramaters and make specifications about analytic choices in order to carry out power calculations.
Or, this may be done retrospectively, which allows researchers to plug in parameter values estimated from data.
Prospectively, researchers may also target a specified level of statistical power (e.g. 80 or 90 percent) and calculate the required sample size for a desired minimum detectable effect size (MDES), which is the smallest true effect size that has a high probability (specified frequently to be 80 or 90 percent) estimated to be be statistically significant at a specified level (for example, at the 0.05 level).
As an "effect size," the MDES is reported in units of standard deviations. For discussion about the concepts and considerations related to MDES, see @RN27978.
Alternatively, researchers may target a specified level of power and calculate the MDES that results for a fixed sample size. 

For a wide variety of RCT designs, researchers can take advantage of closed form solutions and numerous power estimation tools. For example, in education and social policy research, see @RN4473; @RN30153; @RN23884; @RN24179.
In addition to specifications sample size, MDES or power, these estimation tools require the researchers to specify data parameter values (e.g., the explanatory power of baseline covariates inlcuded in the impact model and design choices (e.g. sample sizes, the proportion of units randomly assigned to treatment).
However, an important limitation of the existing methods and tools for estimating statistical power, MDES or sample size requirements for RCTs is that they only apply to the investigation of the effect of an intervention on a *single* outcome.
When researchers investigate the effects on an intervention on *multiple* outcomes, which is more often the case, they may adjust $p$-values upwards to account for the 
multiple hypothesis tests in order to reduce the likelihood of spurious findings of statistically significant effects. 
To do so, researchers may use one of several multiple testing procedures (MTPs). 
An important consequence of using a multiple testing procedure is that there can be a substantial change in the power of the test.
That is, the use of an MTP changes the probability of detecting effects when they truly exist, compared with the situation when the multiplicity problem is ignored.
Unfortunately, while researchers are increasingly using MTPs, they frequently ignore the power implications of their use when designing studies.
Consequently, in some cases sample sizes may be too small, and studies may be underpowered to detect effects as small as a desired size.
In other cases, sample sizes may be larger than needed, or studies may be powered to detect smaller effects than anticipated.
Therefore, it is essential for researchers take the use of MTPs into account when estimating the power, MDES or sample size requirements for their study; however, to our knowledge, no tools exist that take multiple outcomes into account.

With the introduction of the Power under Multiplicity Project (PUMP) package, we fill this gap, relying on validated methods that we have developed for a wide variety of commonly used RCT designs. 
Most of these RCT designs are multilevel designs in which research units are nested in hierarchical groups.
This setting is common in education research, for example, in which students are nested within schools, which can be then nested within school districts.
Though we use educational experiments as a running example throughout this paper, the problem of power estimation for multilevel RCTs is not exclusive to the educational setting.
Our power calculations assume the user will be analyzing these RCTs using frequentist mixed-effects regression models, containing a combination of fixed or random intercepts and treatment impacts at different levels, as we explain in detail below and in Appendix X. 

In this paper, we the PUMP package.
The three core functions are:
- `pump_power()` calculates power given a design, parameters, and minimum detectable effect size
- `pump_mdes()` calculates minimium detectable effect size given a target power
- `pump_sample()` calculates required sample size given a target power and minimum detectable effect size

In addition to these core functions, grid and update functions are built on top of the core power function to allow users to easily explore power over a range of possible values of parameters to determine sensitivity of their power calculations to their assumptions.
Finally, PUMP also visualizes such results graphically.

As will be explained below, the PUMP package allows users to estimate statistical power, MDES or sample size requirements for multiple definitions of statistical power that arise when investigateing effects on multiple outcomes, for multiple designs and model assumptions, and for multiple widely used MTPs: Bonferroni, Holm, Benjamini-Hochberg, and single-step and step-down versions of Westfall-Young.

In the sections below, we provide a summary of the multiple testing problem, discuss how MTPs protect against spurious impact findings and describe the different strategies different MTPs take, introduce our methodology for estimating power when taking the use of MTPs into account and our extensions of the methodology to estimate MDES or power, provide a detailed overview of the PUMP package, walk through an example of using PUMP in real-world example RCT, provide guidance on specifying parameters for power, MDES and sample size requirements and conclude with suggestions for best practices.  

# The multiple testing problem

Researchers are often interested in testing the effectiveness of an intervention on multiple outcomes.
The resulting multiplicity of statistical hypothesis tests can lead to spurious findings of effects.^[Testing the effectiveness of an intervention for multiple subgroups, at multiple points in time, or across multiple treatment groups also results in a multiplicity of statistical hypotheses and can also lead to spurious findings of effects, but this is beyond the scope of this paper.]
Multiple testing procedures (MTPs) are statistical procedures that counteract this problem by adjusting p-values for effect estimates; generally, $p$-values are adjusted upward to require a higher burden of proof.
When not using an MTP, the probability of finding false positives increases, sometimes dramatically, with the number of tests.
When using an MTP, this probability is reduced.

Researchers typically worry that moving from one to multiple hypothesis tests, and thus employing MTPs, results in a loss of power.
However, that pattern need not always be the case.
Power is indeed lost if one focuses on individual power --- the probability of detecting an effect of a particular size or larger for each particular hypothesis test, given that the effect truly exists.
However, in studies with multiplicity, alternative definitions of power exist and in some cases may be more appropriate (@RN23882; @RN23878; @RN23881; Westfall, Tobias, & Wolfinger, 2011).

<!-- KH comment: I would highlight this paragraph more, as it is something we return to throughout the paper. Right now, it seems like a side note. For example, you say one "might" consider complete power, as if we may choose not to in the future, but we do want to consider complete power! Is this worth making a bulleted list, or bolding each term, etc?-->
<!-- KP response: See my added text. I think it's accurate to say that a research might choose to consider complete power as the focus for their study - but they rarely would for power reasons, but we want to be able to compute it so they are paying attention to it or can compute it if they really do want to design their study for complete power-->

For example, when testing for effects on multiple outcomes, one might consider 1-minimal power: the probability of detecting effects of at least a particular size (which can vary by outcome) on at least one outcome.
Similarly, one might consider $1⁄2$-minimal power: the probability of detecting effects of at least a particular size on at least 
$1⁄2$ of the outcomes.
Also, one might consider complete power: the power to detect effects of at least a particular size on all outcomes.
<!-- KH comment: can we get more specific in the next sentence, i.e. provide examples of when we might want to measure success in different ways?.-->
<!-- KP: See added text below-->
The choice of definition of power depends on the objectives of the study and on how the success of the intervention is defined.
<!-- KH comment: the following sentence is vague, I'm not really sure what it is getting at.-->
The choice of definition also affects the overall extent of power.

The prevailing default in many impact studies in the social sciences, individual power, may or may not be the most appropriate type of power.
In some cases, it may provide misleading estimates of the probability that researchers will be able to find sufficient evidence that an intervention was successful. If the researchers’ goal is to find statistically significant estimates of effects on all primary outcomes of interest, then even after taking multiplicity adjustments into account, estimates of individual power can grossly understate the actual power required - complete power.
On the other hand, if the researchers’ goal is to find statistically significant estimates of effects on at least one or on a small proportion of outcomes, then their power may be much better than anticipated.
They may be able to get away with a smaller sample size, or they may be able to detect smaller effect sizes.
The choice of power definition may not be a simple one.
First, it may not be easy to define the success of an intervention. Even when it is easy, aligning the definition of success with a definition of power may not always be.
For example, even if a program would be considered successful should an effect of a specified size (in units of standard deviations) be found for at least one outcome, researchers may still want sufficient individual power because they want to know the probability of detecting effects on each particular outcome.
It may be best for researchers to estimate and share power estimates for multiple power definitions. For example, consider the case in which a sample size is fixed. The probability of detecting statistically significant effects (at least as large as specified effect sizes) may be unacceptably low.
While complete power may be a goal in this case, it may be valuable for researchers to also be able to say that it is still tenable to achieve a high probability of detecting effects on at least half of the outcomes.



<!-- KH comment: do we need to motivate this more, or at least note that there are other options?-->
Our methods focus on the frequentist framework of hypothesis testing, as it is currently the prevailing framework in education and other social policy research.

<!-- KH COMMENT: motivate why we are going into this discussion, i.e. remind reader why we need these details in order to calculate power-->
<!-- KH COMMENT: for non-education people, effect sizes might not be a familiar concept. Maybe this is the right place to introduce it?-->
<!-- KP: I introduced in edits to very start of paper - see what you think -->
Consider that a researchers in interested in testing the impact of an intervention on $M$ outcomes.
In the frequentist framework, when framing impacts in terms of effect sizes ($ES$), for the $m^{th}$ outcome, one typically tests a null hypothesis of no effect, $H_{0_m}: ES_m = 0$, against an alternative hypothesis $H_{1_m}: ES_m \neq 0$ for a two-sided tests or $H_{1_m}: ES_m > 0$ or $H1_m: ES_m < 0$ for a one-sided test.
For the purpose of calculating power, researchers specify an alternative hypothesis of at least a particular effect size.
For example, researchers may specify an ES of $0.125$.
A significance test, such as a two-sided or one-sided t-test, is then conducted, and one obtains a test statistic given by
\begin{equation}
t_m = \frac{\hat{ES}_m}{SE(\hat{ES}_m)},
\end{equation}

from which a raw p-value is computed.
Here, the term "raw" is used to distinguish this p-value from a p-value that has been adjusted using a procedure for multiple hypothesis tests, as discussed below.
The raw p-value is the probability of a test statistic being at least as extreme as the one observed, given that the null hypothesis is true.
<!-- KH COMMENT: I'm not sure if we currently actually allow for one-sided tests, do we want to?-->
<!-- KH COMMENT: we have not defined capital $T_m$-->
For a two-sided test, which is the focus of the discussion going forward (although the PUMP package also allows for one-sided tests), the raw p-value for test $m$ is $p_m=2*Pr(T_m \geq |t_m|)$.^[For a one-sided test, depending on the direction of our alternative hypothesis, the raw p-value for test $m$ is computed as $p_m=Pr(T_m \leq t_m)$ or $p_m=Pr(T_m \geq t_m)$.]
<!-- KH COMMENT: I -->
This expression means we use our knowledge of the sampling distribution of the t-statistic, and we identify where our observed test statistic falls in that distribution when it is centered around zero.
<!--To calculate the raw $p$-value, we use our knowledge of the sampling distribution of the t-statistic, and we identify where our observed test statistic falls in that distribution.-->

When testing a _single_ hypothesis under this framework (effects are being assessed on just one outcome, so that $M=1$), researchers typically specify an acceptable maximum probability of making a Type I error, $\alpha$.
A Type I error is the probability of erroneously rejecting the null hypothesis when it is true.
The quantity $\alpha$ is also referred to as the significance level.
If $\alpha=0.05$, then the null hypothesis is rejected if the p-value is less than $0.05$, and it is concluded that the intervention had an effect because there is less than a $5$% chance that this finding is a false positive.

When one tests _multiple_ hypotheses under this framework (such that $M>1$) and one conducts a separate test for each of the hypotheses with $\alpha=0.05$, there is a _greater_ than $5\%$ chance of a false positive finding in the study.
If the multiple tests are independent, the probability that at least one of the null hypothesis tests will be erroneously rejected is 1-Pr(none of the null hypotheses will be erroneously rejected) $= 1-(1-\alpha)^M$.
Therefore, if researchers are estimating effects on three outcomes, and if these outcomes are assumed independent, the probability of at least one false positive finding is $0.14$.
If the researchers were instead estimating effects on five independent outcomes, the probability of at least one false positive finding is $0.23$.
This Type I error inflation for independent outcomes demonstrates the crux of the multiple testing problem.
In practice, however, the multiple outcomes are usually at least somewhat correlated, which makes the test statistics correlated and reduces the extent of Type I error inflation.
Nonetheless, any error inflation can still make it problematic to draw reliable conclusions about the existence of effects.
<!-- KH comment: WY can actually occassionally adjust p-values down-->
As introduced above, to counteract the multiple testing problem, MTPs adjust p-values upward.^[Alternatively, MTPs can decrease the critical values for rejecting hypothesis tests. For ease of presentation, this paper focuses only on the approach of increasing p-values.]
The following paragraphs describe how using a multiple testing procedure protects against false positives.

Recall that the power of an individual hypothesis test is the probability of rejecting a false null hypothesis of at least a specified size.
If raw p-values are adjusted upward, one is less likely to reject the null hypotheses that are true (meaning there is truly no effect of at least a specified size), which reduces the probability of Type I errors, or false positive findings.
Reducing this probability is the goal of MTPs.
However, if raw p-values are adjusted upward, one is also less likely to reject the null hypotheses that are false (meaning there truly is an effect of at least a specified size).
<!-- Rejecting a null hypothesis that is false is also known as a false negative--we fail to detect a true effect.-->
Therefore, all MTPs reduce individual power (the power of an individual hypothesis test) compared with the situation when no multiplicity adjustments are made or the situation when there is only one hypothesis test.

MTPs also reduce all other definitions of power compared with the situation when no multiplicity adjustments are made -- but not necessarily compared with the situation when there is only one hypothesis test.
For example, $1$-minimal power, the probability of detecting effects (of at least a specified size) on at least one outcome -- after adjusting for multiplicity -- is typically greater than the probability of detecting an effect of the same size on a single outcome.
This increase may or may not occur with other definitions of power (e.g., the probability of detecting a third, half, or all false null hypotheses).

# Using MTPs to protect against spurious impact findings

The MTPs that are the focus of this paper fall into two different classes.
The first class reframes Type I error as a rate across the entire set or “family” of multiple hypothesis tests.
This rate is called the familywise error rate (FWER; @RN33098).
The FWER is typically set to the same value as the probability of a Type I error for a single test, or to $\alpha$.
<!--The FWER is typically set to the same value as the probability of a Type I error for a single test, e.g. $\alpha$.-->
MTPs that control the FWER at $5\%$ adjust p-values in a way that ensures that the probability of at least one Type I error across the entire set of hypothesis tests is no more than $5\%$.
The MTPs introduced by Bonferroni (@RN24280, @RN24281), @RN24282, and @RN28696 control the FWER.

The second class of MTPs takes an entirely different approach to the multiple testing problem.
MTPs in this class control the false discovery rate (FDR).
<!-- FDR is a less stringent criteria than FWER.-->
Introduced by Benjamini and Hochberg (1995), the FDR is the expected proportion of all rejected hypotheses that are erroneously rejected.
The two-by-two representation in Table 1 is often found in articles on multiple hypothesis testing, and helps to illustrate the difference between FWER and FDR.
Let $M$ be the total number of tests.
Therefore, we have $M$ unobserved truths: whether or not each null hypothesis is true or false.
We also have $M$ observed decisions: whether or not the null hypotheses were rejected, because the p-values were less than $\alpha$.
In Table 1, A, B, C and D are four possible scenarios: the numbers of true or false hypotheses not rejected or rejected. 
$M0$ and $M1$ are the unobservable numbers of true null and false null hypotheses.
$R$ is the number of null hypotheses that were rejected, and $M - R$ is the number of null hypotheses that were not rejected.

<!-- KH comment: maybe different notation because we use $B$ for WY?-->
<!-- KP: good catch, we can maybe use C, D, E, F? I don't know how to draw this table in markdown-->
In Table 1, $B$ is the number of erroneously rejected null hypotheses, or the number of false positive findings.
Therefore, the FWER is equivalent to $Pr(B > 0)$, the probability of at least one false positive finding.
Recall the examples above about Type I error inflation when testing for effects on independent outcomes in the case that $\alpha$ is set to $0.05$ and no MTPs are applied.
<!-- Recall that Type I error is inflated when testing for effects on independent outcomes when no MTPs are applied.-->
The Type I error was almost $10\%$ when testing effects on two independent outcomes and $23\%$ when testing effects on five independent outcomes.
These Type I error rates both correspond to the FWER.
The goal of MTPs that control the FWER is to bring these percentages back down to $5\%$. 

Also in Table 1, the FDR is equal to $E(\frac{B}{R})$ but is defined to be $0$ when $R=0$, or when no hypotheses are rejected.
As is frequently noted in the literature (e.g., @RN352; @RN23748), the FWER and FDR have different objectives.
Control of the FWER protects researchers from spurious findings and so may be preferred when even a single false positive could lead to the wrong conclusion about the effectiveness of an intervention.
On the other hand, the FDR is more lenient with false positives.
Researchers may be willing to accept a few false positives, $B$, when the total number of rejected hypotheses, $R$, is large.
Note that under the complete null hypothesis that all $M$ null hypotheses are null, the FDR is equal to the FWER, because when referring back to Table 1 we have $FWER=P(R>0)=E(\frac{B}{R})=FDR$.
However, if any effects truly exist, then $FWER \geq FDR$.

<!-- KH comment: this paragraph isn't cohesive to me-->
As a result, in the case where there is at least one false null hypothesis (at least one true effect at least as large as a specified effect size), an MTP that controls the FDR at $5\%$ will have a Type I error rate that is greater than $5\%$. 
<!--As a result of the difference in objective between FWER and FDR, in the case where there is at least one false null hypothesis (at least one true effect at least as large as a specified effect size), an MTP that controls the FDR at $5\%$ will have a Type I error rate that is greater than $5\%$.-->
Note that MTPs may provide either weak or strong control of the error rate they target.
An MTP provides weak control of the FWER or the FDR at level $\alpha$ if the control can only be guaranteed when all nulls are true, or when the effects on all outcomes are zero.
An MTP provides strong control of the FWER or FDR at level if the control is guaranteed when some null hypotheses are true and some are false, or when there may be effects on at least some outcomes.
Of course, strong control is preferred.^[The single-step and step-down Westfall Young MTPs always provide at least weak control of the FWER.
In order for these procedures to provide strong control of the FWER, they require the assumption of subset
pivotality (Ge, Dudoit, & Speed, 2003). The distribution of the unadjusted test statistics or p-values is said to
have subset pivotality if for any subset of null hypotheses, the joint distribution of the test statistics or of the pvalues for the subset is identical to the distribution under the complete null. A consequence of this assumption
is that the resampling of test statistics or p-values can be done under the complete null hypothesis rather than
under the unknown partial hypothesis (Ge et al., 2003)]


The list of multiple testing procedures supported by the PUMP package can be found below.

```{r, echo = FALSE}
info <- pump_info()
```

```{r, echo = FALSE}
kable(info$Adjustment)
```

# Estimating power, MDES and sample size in studies with multiple outcomes

## Power estimation strategy

We take an innovative simulation-based approach to estimating power for multiple outcomes, as introduced in @Porter2018.
In order to estimate power for a single outcome, we can often use closed-form algebraic expressions, which are derived from the assumed model.
However, with multiple outcomes, finding such expressions can be quite difficult, or even impossible depending on the multiple testing procedure.
In cases where it is possible to find a closed-form expression, we would need to find expressions for every design, MTP, and definition of power.
Importantly, we would also need to find new expressions for each number of outcomes, which quickly becomes an intractable problem!
In some cases, such as permutation-based procedures like Westfall-Young approaches, a closed-formed solution does not exist.
Instead, we rely on simulation to calculate estimated power.
The approach outlined below can estimate power for any scenario.

If we were to rely on a full simulation approach, we could use the following method to estimate power:

<!-- KP COMMENT: I INSERTED SOME EDITS AND COMMENTS BELOW ABOUT WAYS WE MIGHT EXPAND WITH A FEW MORE DETAILS. BUT I'M NOT WED TO THIS APPROACH - SEE WHAT YOU THINK AND WE CAN DISCUSS. -->

<!-- KH response: I find the proposed changes to this particular section a bit confusing. The goal here is not to discuss the validation, but instead to discuss what we actually do in the PUMP package. I mention simulating data as what we *could* do, but then immediately say that we skip that step, and go straight to generating the test statistics. So it feels like it would confuse the reader to go into a lot of detail about how we could calculate the power using simulated data, but then turn around and say those details don't matter because we don't use them! -->

1. *Simulate a data sample according to the joint alternative hypothesis.*
First, we formulate what we will refer to as the *joint alternative hypothesis*, which is the set of outcomes we assume to have nonzero treatment effects.
We define $\psi_m$ to be the treatment effect for outcome $m$, with $M$ total outcomes.
If we have $M = 3$ treatments, one possible joint alternative hypothesis is that all outcomes have nonzero effects: $H_A: \psi_1 \neq 0, \psi_2 \neq 0, \psi_3 \neq 0$.
Another possible joint alternative hypothesis is that only the first two outcomes have nonzero effects: $H_A: \psi_1 \neq 0, \psi_2 \neq 0, \psi_3 = 0$.
Then, we would generate simulated data under the joint alternative hypothesis.

<!--KP comment: I wonder if it would be helpful here to list the set of parameters here and their notation (drawing from your documentation for the MC simulations). This could be a place to introduce this notation for referring back to? Also, I took out the text about nonzero effects because we don't neccessarily simulate the data assuming the alternative for all M hypotheses, but rather the joint alternative (if that terminology is right or makes any sense) such that some null hypotheses are assumed to be false but some are assumed to be true. -->

<!-- KH response: I'm a bit worried that putting that detail in this spot in particular would confuse the reader. This step is introduced but then immediately afterward we say that we skip it! So it feels a bit weird to say 'here's how we would do a full simulation, but we don't actually do it here.' -->
<!-- I'm open to taking a different approach. Let's discuss-->
2. *Calculate test statistics $t_1$ under the joint alternative hypothesis.*
Given simulated data, for example we could fit $M$ regression models (specified to match model assumptions).
<!--KH response: Because we don't actually simulate data, we don't fit regression models!! -->
<!--KP: I'm confused - we do fit regression models - that's the most time-intensive part-->
3. *Calculate adjusted $p$-values.*
The test statistics can be used to compute raw (unadjusted) $p$-values.
<!--KH comment: similar to my point above--in this setting we are not using regression functions -->
4. *Repeat above steps (1 through 3) for a large number of iterations.*
Denote the number of iterations $tnum$. Repeating steps $1$-$3$ $tnum$ times results in a matrix of unadjusted p-values which we call $\mathbf{F}$, and is of dimension $tnum \times M$.
5. *Adjust p-values.*
For each row, corresponding to one simulated dataset, the $M$ raw p-values corresponding to the $M$ hypothesis tests can be adjusted according to the desired multiple testing procedure to generate a new matrix $\mathbf{G}$.
For Bonferroni, Holm, and BH adjustments, we use the function `p.adjust` in R (found in the `stats` package).
We developed our own function for implementing adjustment using the Westfall-Young procedures.
6. *Calculate hypothesis rejection indicators.*
For each MTP, the matrix of adjusted p-values $\mathbf{G}$ can then be compared with a specified value of $\alpha$ (the default is $0.05$, but the value can be changed by the user).
For each row, which is one iteration of simulated data, we record whether or not the null hypothesis was rejected for each outcome.
This process results in a new matrix $\mathbf{H}$, which contains hypothesis rejection indicators, and is still of dimension $tnum \times M$.
Using $\mathbf{H}$, we can compute all definitions of power.
7. *Calculate power.*
To compute the different definitions of power:
    - *Individual power for outcome $m$* is the proportion of the `tnum` rows in which the null hypothesis $m$ was rejected (the mean of column $m$ of $\mathbf{H}$).
    - *$d$-minimal power* is the proportion of the $tnum$ rows in which at least $d$ of the $M$ null hypotheses were rejected.^[Note that others refer to 1-minimal power simply as “minimal power” (e.g., @RN33095; @RN23882; Westfall, Tobias, & Wolfinger, 2011), “disjunctive power” (e.g., Bretz, Hothorn, & Westfall, 2011), or “any pair” power (@RN33097). @RN23882 use the terminology of “r-power” for what is refered to here as d-minimal power for d>1.]
    -  *Complete power* is the proportion of the $tnum$ rows in which all of the null hypotheses were rejected based on the raw p-values rather than adjusted $p$-values.

<!--KH comment: I find the below explanation of complete power a bit short and confusing. -->
<!--KP: me too! but I don't understand it sufficiently to say much more. Luke, can you assist? -->

The reason that complete power is based on raw p-values is that the probability of all tests having a raw p-value less than $0.05$ when the null hypothesis is true is less than the probability that any single test would have a p-value less than $\alpha$ by chance (@RN23880; Westfall et al., 2011).^[Complete power does not in itself require unadjusted tests. The above approach for not adjusting tests assumes that all tests must to be statistically significant in order to claim impacts on all outcomes.]
 
Above, we outline a full simulation-based approach for calculating power.
We can simplify this process by skipping the first step.
<!-- We skip steps 1 through 4-->
Given an assumed model and correlation structure for the test statistics, we can directly sample from $f(t_1)$, the joint alternative distribution of the test statistics.
This shortcut vastly improves both the simplicity and the speed of computation.
In summary, our approach is:

1. **Sample** *test statistics $t_1$ under the joint alternative hypothesis.*
<!-- KP: I'm not sure I understand what this means here to say "sample." We generate test statistics. Is it accurate to say those are a sample of test statistics, or is it that we generate test statistics from a large number of data samples?-->
3. *Calculate adjusted $p$-values.*
4. *Repeat above steps (1 through 2) for a large number of iterations and calculate power.*
<!-- KP: We don't repeat. We do it all in one swoop by generating the test statistics. And then we have to do steps 6 and 7 (but acutally # is step 5 above--> 

We now describe how to sample from $f(t_1)$ directly.
First, we assume a particular research design and model.
Define $\psi_m$ as the treatment effect for outcome $m$.
Then, we can also express the treatment effect in terms of effect size:
$$ES_m = \frac{\psi_m}{\sigma_{m}}$$ 
where $\sigma_{m}$ is the standard deviation of outcome $Y_m$.
In order to calculate power, we are interested in the standard error of the estimated effect size, which we denote as 
$$Q_m = SE(\hat{ES}_m).$$
The quantity $Q_m$ is defined by the assumed model, and can be a function of the number of units at different levels, the percent of units treated, the assumed $R^2$, and other parameters.
When analyzing actual data, we do not know the true value of $Q_m$, so we would need to estimate $Q_m$ by plugging in either known or estimated values of the relevant parameters.
Some parameters, such as the percent of units treated, are known, while others, such as the $R^2$ at different levels, would need to be estimated.

Given an estimate of $\hat{Q}_m$, we can arrive at the distribution of test statistics.
When testing the hypothesis for outcome $m$, the test statistics for a $t$ test is:
$$t_m = \frac{\hat{ES}_m}{\hat{Q}_m}$$
with degrees of freedom $df$, also defined by the assumed model.
Under the alternative hypothesis for outcome $m$, $t_m$ has a $t$ distribution with degrees of freedom $df$ and mean $\hat{ES}_m/\hat{Q}_m$.
Finally, we choose the correlation matrix between test statistics $\rho$ to sample from the joint distribution of $t_m, m = 1, \dots M$.

From the power formulas, we can then also arrive at MDES and sample size calculations.
From @RN4473, in general the MDES can be estimated as
$$ MDES = MT_{df} \times SE / \sigma_{m} $$

where $MT_{df}$ is known as the multiplier and is the sum of two $t$ statistics with  degrees of freedom $df$.
For one-tailed tests, $MT_{df} = t_{\alpha}^\star + t_{1-\beta}^\star$ where $\alpha$ is the type I error rate and $\beta$ is the desired power. 
For two-tailed tests, $MT_{df} = t_{\alpha/2}^\star + t_{1-\beta}^\star$.
We do not explain the details of the derivations here; for more details and understanding, see @RN4473 or @RN27978.
Manipulating this expression then results in sample size formulae.

<!--KP COMMENT: MAYBE UNPACK THIS/EXPLAIN A BIT HERE? . IT MIGHT BE NICE TO COMMENT HERE HOWEVER THAT BECAUSE WY REQUIRES THIS RESAMPLING, THE COMPUTATIONAL INTENSITY OF IMPLEMENTING IT ALONG WITH FULL MC SIMS IS RIDICULOUS. -->
<!--KH response: good idea, I added some more detail!-->

The $p$-value adjustment using Westfall-Young procedures is the most complex.
We briefly outline the algorithm below.
Similar to above, we first explain a full simulation approach, and then discuss our simplification.
Under a full simulation approach, we would first generate a single dataset under the joint alternative hypothesis and calculate a set of observed test statistics.
Then, we would resample the simulated data, say $B = 3,000$ times, under the joint null hypothesis, and calculate test statistics on each of these resampled datasets to generate a distribution of test statistics under the joint null distribution.
Next, we compare the distribution of observed test statistics to the distribution of test statistics under the joint null distribution to calculate $p$-values.
We would then re-generate a new simulated dataset, and repeat the process. 
If we were to generate $tnum = 10,000$ datasets under the joint alternative hypothesis, for each of these datasets we also generate $B = 3,000$ resampled datasets under the joint null, so we would have to generate $10,000 \times 3,000$ datasets!

When we skip the simulation step, for each iteration $t$ in $1, \dots, tnum$ we generate a set of observed test statistics from the joint alternative distribution.
Then, we augment the method by drawing $B$ samples of test statistics under the joint null rather than resampling the data $B$ times.
Under the null hypothesis, $t_m$ has a $t$ distribution with degrees of freedom $df$ and mean $0$.
As before, we then compare the distribution of observed test statistics to the distribution of test statistics under the joint null distribution to calculate $p$-values.
Westfall-Young procedures are computationally intensive, so the approach of skipping the simulated data step is particularly helpful here.
This approach substantially reduces computational time by drawing test statistics directly rather than resampling data.
Additionally, we find that sampling the test statistics can actually produce more accurate results in some cases for WY procedures; see TODO appendix for more details.

Note that this approach of simulating test statistics builds on work by @RN33089, who use simulated test statistics to identify critical values based on the distribution of the maximum test statistics.
Their approach produces the same estimates as the approach described here for the single-step Westfall-Young MTP.
@RN23882 derived explicit formulas for $d-$minimal powers of stepwise procedures and for complete power of single-step procedures, but only for 1, 2, or 3 tests.
The approach presented here is more generally applicable, as it can be used for all MTPs, for any number of tests, and for all definitions of power discussed in the present paper.

## Randomized Control Trial Designs and Models

<!--KP COMMENT: I WONDER IF YOU MIGHT TAKE THE TEXT FROM "POWER ESTIMATION METHODS" HERE. MAYBE ALL OF SECTION 1? OR IS THAT TOO MUCH DETAIL? THEN SECTIONS 2-7 CAN BE AN APPENDIX?-->
<!--KH response: I've done another pass through and I think now most of the material in Section 1 is summarized here.  Let me know if you think anything is important here and still missing! -->

When designing a study, the researcher has two main choices.
First, the researcher chooses the design of the experiment, including the number of levels, and at which level randomization occurs.
Second, and separately, the researchers choose an assumed model, including whether intercepts and treatment effects should be treated as constant, fixed, or random.
For the same experimental design, the analyst can make sometimes choose from a variety of possible models, and these two decisions should be conceptually separated from each other.

For the design, the PUMP package supports designs with 1, 2, or 3 levels, with randomization occurring at any level.
For example, a design with 2 levels and randomization at level 1 is a blocked design.
A design with 3 levels and randomization at level 3 is a cluster design.
For modeling, we have the following choices.

- Whether level 2 and level 3 intercepts are:
  - fixed: intercepts are fixed effects constrained to have mean 0.
  - random: intercepts are considered to be Normally distributed, allowing for partial pooling.
- Whether level 2 and level 3 treatment effects are:
  - constant: all units in a level have the same single average impact.
  - fixed: each unit within a level has an individual estimated impact, with an additional mean impact.
  - random: treatment impacts are Normally distributed around a mean impact.
  
The research design is denoted by $d$, followed by the number of levels and randomization level, so `d3.1` is a $3$-level design with randomization at level $1$.
The model is denoted by $m$, followed by the level and the assumption for the intercept, either $f$ or $r$ and then the assumption for the treatment impacts, $c$, $f$, or $r$.
For example, `m3ff2rc` means at level $3$, we assume fixed intercepts and treatment impacts, and at level $2$ we assume random intercepts and constant treatment impacts.
The full design and model are specified by concatenating these together, e.g. `d3.2_m3ff2rc`.

The full list of supported models is below.
We also including the corresponding names from the PowerUP! package where appropriate.
For more details about each model, see the appendix.

```{r, echo = FALSE}
kable(info$Design[,1:4])
```

### Understanding design parameters

The table below shows the parameters that influence $Q_m$ formulae for different designs.

```{r, echo = FALSE}
kable(info$Parameters)
```

A few parameters warrant more explanation.
<!-- KP: This is good. I assume this will be expanded. Is this where we might discuss how to think about the correlation between test statistics and discuss how the package can help with the simulations?-->
<!-- KP: We can bullet these out. I can help expand list but don't have time now.-->
The quantity $\text{ICC}$ is the Intraclass Correlation, and gives a measure of variation at different levels of the model.
For each outcome, the ICC for each level is defined as the ratio of the variance at that level divided by the overall variance of the individual outcomes.
The ICC is for the unconditional model, and therefore include the variation due to covariates.
For each outcome, the quantity $\omega$ for each level is the ratio between impact variation at that level and mean variation at that level.
The $R^2$ expressions are the percent of variation at a particular level predicted by covariates specific to that level.

## Estimating MDES and sample size

Frequently, a researcher's main concern with power is prospectively calculating either the minimum detectable effect size (MDES) from a possible study, or determining the necessary sample size.
Given our simulation approach, it is necessary to perform a search algorithm to calculate MDES and sample size.
The user provides a particular target power, say $80\%$.
To perform a search, we calculate power over a range of different MDES or sample size values, and then find the value (of sample size of MDES) that is within a specified tolerance of the target power.
We discuss the algorithm for sample size, although the approach for MDES is the same.

First, we begin by bounding the possible range of sample size values.
Bonferroni is the most conservative correction, so it would result in the largest possible sample size, and thus a Bonferroni correction provides our upper bound.
If we are interested in complete power, we have a larger upper bound; in order to have complete power of $0.8$, we would need each outcome to have an individual power of $\text{0.8}^{(1/M)}$.
On the other hand, our least conservative correction is doing no correction at all, and would result in the smallest possible sample size, so no adjustment provides our lower bound.
If we are interested in minimal power, we must have a smaller lower bound; in order to have 1-minimal power of $0.8$, each outcome needs to have individual power of $1 - (1 - \text{0.8})^{(1/M)}$.

Second, given our lower and upper bounds, we now use optimization to find the sample size.
**TODO** Describe the optimization procedure. (Luke?)

Finally, given a proposed sample size value by the optimization procedure, we calculate the power for the given power to ensure it is within a specified tolerance of the target power.
The default tolerance is $1%$, so given a target power of $80%$, we check whether the given sample size is between $79%$ and $81%$.

## Validation

We completed extensive validation checks to ensure our power calculation procedure was correct.
First, we compared our power estimates in scenarios with only one outcome, $M = 1$, to PowerUpR!.
Without a multiple testing procedure adjustment, our estimates match.
Second, in order to validate our estimates under multiplicity, we used a simulation approach.
We generated many iterations of data according to the assumed design and model, calculated p-values, and calculated an empirical estimate of power.
Finally, using a binomial distribution we construct confidence intervals for the power estimate.
Then, we validated that the PUMP estimates fall within these confidence intervals.

A more detailed explanation of the validation procedure can be found in the Appendix, and full validation code and results are in our github repository **TODO**.
For some scenarios, we have discrepancies from PowerUp resulting from different modeling choices.
For example, for certain models PowerUp assumes the intraclass correlation is zero, while we allow for nonzero values.
When there are discrepancies, these are noted in the appendix.


# The PUM-P Package


To illustrate the `pum-p` package we conduct a power analysis for a blocked cluster-randomized RCT (this is a three level design with the random assignment at level two).
In particular, we follow the general design of the Diplomas Now evaluation conducted by MDRC (see, e.g., https://www.mdrc.org/project/diplomas-now#overview).

As taken from the above site, Diplomas Now "is a secondary school model focused on meeting the holistic needs of all students in grades six through twelve. It is designed to be robust and intense enough to transform or turn around high-poverty and high-needs middle grade and high schools attended by many students who fall off the path to high school graduation. Diplomas Now combines programming developed by each of the three organizations that created it: Talent Development, City Year, and Communities In Schools."
Diplomas Now, with MDRC as a partner, was one of the first validation grants awarded as part of the i3 competition administered by the federal Department of Education.

For the experiment, "62 secondary schools in 11 school districts agreed to participate in this study between 2011 and 2013. 32 of these schools were randomly assigned to implement the Diplomas Now model while the other 30 schools were assigned to a control group, continuing their existing school programs or implementing other reform strategies of their choosing." (Details quoted from the "Addressing Early Warning Indicators: Interim Impact Findings from the Investing in Innovation (i3) Evaluation of DIPLOMAS NOW" report.)

The Diplomas Now study covered both middle and high schools, and was rolled out over a series of years.
The designers therefore blocked the schools by district, school type, and year of roll-out.
After having to drop some schools due to various reasons, the evaluators were left with 29 high schools and 29 middle schools grouped in 21 random assignment blocks.

<!-- KH comment: this seems to assume a decent level of familiarity with multi-level designs. I assume that is our audience, but would it be helpful to provide a quick review just in case? For example, do non-education fields talk about hierarchical designs with the same terminology? I still get confused as to the difference for example between a block and a cluster.

LWM: Not sure what you mean by the above comment
-->

We have three levels of data: 21 blocks (level 3) of a few schools (level 2) in each block, with students (level 1) in the schools.
The schools are _clusters_, and are the units we randomly assign to treatment and control.
We randomly assign _within block_, meaning each block is in effect a mini-experiment with a pre-designated proportion of schools treated.
Under our naming, this configuration is a three level design with randomization at level two, or a "d3.2" design.

To calculate power, we also need to specify how we will analyze our data.
Following the original report, we plan on using a multilevel model (a common choice for cluster randomized experiments, and especially common in education) with fixed effects at level 3, including a treatment by block interaction term, a random intercept at level 2, and a single treatment coefficient for each block of schools.
We represent this model as "m3fc2rc."
The "3fc" means we are including block fixed effects, and not modeling any treatment impact variation at level 3.
The "2rc" means random intercept and no modeled variation of treatment within each block (the "c" is for "constant").

The reason we need to account for multiple testing is we have 12 outcomes of interest.
We have three types of primary outcome (Attendance, Behavior, Course performance, called the "ABC's"), along with an ABC composite measure of an indicator of whether a student is above given thresholds on all of the first three measures.
Due to the grouped nature of the outcomes, we elect to do a power analysis separately for each outcome group to control family-wise error rather than overall error.^[We note that there are different guidelines for when to adjust for multiple outcomes in education studies. For example, Schochet (2008) recommends organizing primary outcomes into domains, conducting tests on composite domain outcomes, and applying multiplicity adjustments to composites across domains. The What Works Clearinghouse applies multiplicity adjustments to findings within the same domain rather than across different domains. Our methods apply to either case.] 


## Power of the original design

To calculate power we need to establish the design of the study, the size of the study, and the expected relationships between covariates, outcomes, and units in the study.
All of these numbers have to be determined given resource limitations, or estimated using prior knowledge, pilot studies, or other sources of information.
Regarding size, we assume equal size blocks and clusters, as is typical of most power analysis packages.
For the above, this gives about 3 clusters (schools) per block.
The report states there were 14,950 students, yielding around 258 students per school.
Normally we would use the geometric means of clusters per block and students per cluster as our design parameters, but that information is not available in the report.
We assume 50% of the schools are treated; there will be a bit of a power discrepancy given the small blocks as we cannot treat 50% in odd-sized blocks.

We next turn to generating values for the remaining parameters.
In particular, we need values for the $R^2$ of the possible covariates, the Intraclass Correlation Coefficient (ICC), and an estimate of treatment variation.
The report does not provide these quantities, but it does mention covariate adjustment in the presentation of the model.
Given the outcomes, it is unlikely there are highly predictive individual level covariates, but prior year school-average attendance, etc., is likely to be highly predictive of corresponding school-average outcomes.
We thus set $R^2_1 = 0.1$ and $R^2_2 = 0.5$.
We assume five covariates at level one and three at level two; this decision, especially for level one, usually does not matter much in practice, unless sample sizes are very small.

ICC measures are used to divide overall variation in outcome across levels: e.g., do we see reletively homogenous students within schools that are quite different, or are the schools generally the same with substantial variation within them.
Normally ICCs calculated from pilot data are used or, failing that, ICCs can be pulled from other reports.
We here specify a level two ICC of 0.05, and a level three ICC of 0.40.
We set a relatively high level three ICC to capture the potential impact of blocking, which is designed to isolate variation; in particular we might imagine attendence changes markedly between middle and high school as well as across schools.
We assume there is cross-block treatment variation by setting omega.3 to 0.50 (a substantial amount); most power analyses would assume no variation, we do here for illustration.
For further discussion of selecting these parameters see, for example, *CITE Porter original paper and PowerUp!  Or other design parameter papers??*.

<!-- KH comment: this paragraph again assumes a fair amount of background knowledge, but probably OK

LWM: I agree; not sure how much more we would want to put in.-->

At this point we also need to specify the planned method of analysis.
The report calls their model a "two-level" model, but this is not quite in alignment with the language of this package.
In particular, fixed effects included at level two are actually accounting for variation at level three; we therefore identify their model as a three-level model with fixed effects at level three.

For illustration, we select attendance as our outcome group.
We have three different attendance measures, so we need to adjust across three outcomes.
We must specify the desired minimum detectable effect sizes (MDES) of our treatment impact, in effect size units, for each.
We will calculate power for this hypothesized effect; any larger effects would have higher power.
We initially assume a modest effect of $0.10\sigma$ for all three outcomes.

Within the attendance group, we also have to specify the correlation of our test statistics.
As a rough proxy, we use the correlation of the outcomes at the level of randomization; in our case this would be the correlation of school-average attendance within block.
We believe the attendance measures would be fairly related, so we select `rho = 0.40` for all pairs of outcomes.

Now we have initial values for all needed parameters.
Of course, in a full power analysis, we would explore ranges of values to see how power changes across a range of specifications; we discuss this further below.
We could also specify different values for the $R^2$s and $ICC$s for the different outcomes, if we thought they had different characteristics; for simplicity we do not do this here.
The `pum-p` package also allows specifying different pairwise correlations between the different outcomes via a matrix of $\rho$s rather than a single $\rho$; also for simplicity, we do not do that here.

Given a specific set of parameters, we calculate the power of our design using `pump_power()` as follows:

```{r calc_initial_power}

p <- pump_power( design = "d3.2_m3fc2rc", # choice of design and analysis strategy
            MTP = "Bonferroni", # multiple testing procedure
            MDES = 0.10, # assumed effect size
            M = 3, # number of outcomes
            J = 3, # number of schools/block
            K = 21, # number RA blocks
            nbar = 258, # average number of students per school
            Tbar = 0.50, # prop Tx
            alpha = 0.05, # significance level
            numCovar.1 = 5, numCovar.2 = 3, # number of covariates per level
            R2.1 = 0.1, R2.2 = 0.7, # Explanatory power of covariates for each level
            ICC.2 = 0.05, ICC.3 = 0.4, # Intraclass correlation coefficients 
            omega.3 = 0.50, # Amount of treatment variation at level 3.
            rho = 0.4 ) # how correlated outcomes are
```

The results are easily made into a nice table via `knitr`'s `kable()` command:
```{r echo = FALSE}
kable(p, digits=2)
```


*Results.* The first three columns are the powers for rejecting each of the three outcomes---they are (up to simulation error) the same since we assumed the same MDES for all three.
The `indiv.mean` is just the mean power across all three outcomes.
The first row is power without adjustment, where we see approximately 80% power, and the second row has our power with the listed $p$-value adjustment, with a substantially lower 66% power.

The next columns show different multi-outcome definitions of power.
In particular, `min1` and `min2` show the chance of rejecting at least one or two hypotheses, respectively.
The `complete` is rejecting all three hypotheses.^[The package does not show power for these without adjustment for multiple testing, as that power would be grossly inflated and meaningless.]

Alhough the Bonferroni adjustment does substantially diminish individual power, we still have more than an 80% chance of rejecting at least one null of our three outcomes: while our study will not be well powered for any individual effect, it is more powered than we might expect to detect _some_ effect, even when using the very conservative Bonferroni.


<!--KP comment: I wonder if one slight modification to the set-up is to walk through the recommendations for how an analyst should make decisions about assumptions and run one scenario that they might come to - which you have done above but then illustrate the impact of other assumptions. You've done this but we perhaps spell it out more and include more discussion of the impact different values of different parameters can have.

LWM: Does the following do this?  I am not sure what you mean here.
-->

Given the above, we might wonder how power shifts if we change our parameters.
We can do this with `update()`.
For example, here we examine what happens if the ICCs are more equally split across levels two and three:
```{r}
p_b <- update( p, ICC.2 = 0.20, ICC.3 = 0.25 )
p_b
```

Our assumption that variation was primarily captured in level three matters a great deal for power.

We could also investigate whether having different impacts for different outcomes (in particular no actual impact for one of our outcomes) would impact power.
When estimating power for multiple outcomes, it is important to consider cases where some of the outcomes in fact have null, or very small, effects, to hedge against being underpowered if one of the outcomes is not well measured, for example:
```{r}
p_c <- update( p, MDES = c( 0.15, 0.05, 0.0 ) )
p_c
```

When calculating power for a given scenario, it is easy to vary many of our design parameters by outcomes.  E.g., if we thought we had better predictive covariates for our second outcome, we might have:

```{r}
p_d = update( p, 
              R2.1 = c( 0.1, 0.3, 0.1 ),
              R2.2 = c( 0.4, 0.8, 0.7 ),
              omega.3 = c( 0.4, 0.5, 0.6 ) )
p_d
```

Notice how the individual powers are heavily impacted.  The min-$d$ powers will naturally take the varying outcomes into account as we are calculating a joint distribution of test statistics that will have the correct marginal distributions based on these different design parameter values.

After several `update()`s, we may lose track of where we are; to find out, we can always check details with `print_design()` or `summary()`:

```{r}
summary(p_d) 
```


## Examining other adjustment procedures

<!-- KH comment: as a standalone vignette, the user does not know how these things are being calculated, i.e. that these values are generated via simulation, so that might be a bit surprising ot them!

LWM: But I think  that is ok?
-->

It is easy to rerun the above using the Westfall-Young Stepdown procedure (this procedure is much more computationally intensive to run), or other procedures of interest.
Alternatively, simply provide `pump_power()` a list of procedures you wish to compare.
If you provide a list, the package will re-run the simulation for each item on the list, so the overall method call can get computationally intensive.
Here we update, again, with a full list. We could also have simply provided this list to `pump_power()` initially.

```{r othercorrections, cache=TRUE}
p2 <- update( p, MTP = c("Bonferroni", "Holm", "WY-SD") )
```

```{r echo = FALSE}
kable(p2, digits=2)
```

The more sophisticated (and less conservative) adjustment exploits the correlation in our outcomes (`rho = 0.4`) to provide higher individual power.
We do not see elevated rates for min-1 power, interestingly.
Accounting for the correlation of the test statistics when adjusting $p$-values can drive some power (indivdual power) up, but on the flip side min-1 power can be driven down as the lack of independence between tests gives fewer chances for a significant result.
See *CITE porter* for further discussion; while Porter (2017) focuses on a single experimental design, the lessons learned there apply to all designs as the only substantive difference between the designs is in how we calculate the distribution of the test statistics.



## Alternative methods of estimation

There are usually a range of modeling choices one might bring to a given experimental design.
For example, for multisite experiments ("d2.1" designs), *CITE Miratrix and Weiss* identify 15 different estimation strategies.
Different choices here can imply different targeted estimands, which in turn can impact power.
In particular, methods that target superpopulation averages vs. finite sample averages will generally have lower power if there is treatment impact variation.

In `pum-p` these choices are specified by different `design` arguments.
For our context, for example, we could use a random effects model at level 3 instead of a fixed effects model, setting `design = "d3.2_m3rr2rc"` instead of `"d3.2_m3fc2rc"`; this would target a superpopulation average, viewing the blocks as a random sample, vs. a finite population where the blocks are considered fixed.

Random effects models allow for level 3 covariates, which we would need to specify via `numCovar.3` and `R2.3` to capture how many there are and how predictive they are:

```{r}
p3 <- update( p, design="d3.2_m3rr2rc", numCovar.3 = 3, R2.3 = 0.40 )
p3
```


## Exploring parameter combinations

<!--KP comment: As note earlier, I am wondering if we might expand this section more, looking at the impact of different assumptions across (1) R2; (2) ICC; (3) rho; (4) number of outcomes with impacts; and maybe (5) adjustment procedures. We can include plots from the plot functions we add to our package (or to the grid functions). 
-->

To explore sensitivity to different design parameters, we can call `pump_power_grid`, which will calculate power on all combinations of a set of passed parameter values.
For some discussion of what parameters will affect power more generally, see <<PowerUp paper?>>.
For discussion of how design parameters can affect the overall power in the multiple testing context, especially with regards to the overall power measures such as min1 or complete power, see the discussion in <<Kristen paper>>; the findings there are general, as they are a function of the final distribution of test statistics.
The key insight into the simulation approach is that power is a function of the individual-level standard errors and degrees of freedom, and how correlated the test statistics are; once we have these elements, regardless of the design, we can proceed.

To illustrate, we consider three common areas of exploration: Intraclass Correlation Coefficients (ICCs), the correlation of test statistics, and the assumed number of non-zero effects.
The last two are particularly important for multiple outcome contexts.

### Exploring the impact of the ICC

We can explore a range of options for both level two and three ICCs if want to ensure our power is sufficient across a set of plausible values.
The `update_grid()` call makes this straightforward: we pass our baseline scenario along with lists of parameters to additionally explore:

```{r, cache=TRUE, fig.height=3, fig.width=7}
grid <- update_grid( p,
            ICC.2 = seq( 0, 0.3, 0.05 ),
            ICC.3 = seq( 0, 0.60, 0.20 ),
            tnum = 5000 ) 

grid$ICC.3 = as.factor( grid$ICC.3 )
grid = filter( grid, MTP == "Bonferroni" )
ggplot( grid, aes( ICC.2, min1, group = ICC.3, col = ICC.3 ) ) +
  geom_line() + geom_point()
```

We see that higher ICC.2 radically reduces power to detect anything and ICC.3 does little.
To understand why, we turn to our standard error formula for this design and model:
$$
\begin{aligned}
SE( \hat{\tau} ) = \sqrt{
\frac{\text{ICC}_{2}(1 - R^2_{2})}{\bar{T}(1 - \bar{T}) JK} +
\frac{(1-\text{ICC}_{2} - \text{ICC}_{3})(1-R^2_{1})}{\bar{T}(1 - \bar{T}) J K\bar{n}} } .
\end{aligned} 
$$
In the above, the $\bar{n} = 258$ students per group makes the second term very small compared to the first regardless of the ICC.3 value.
The first term, however, is a direct scaling of ICC.2; changing it will change the standard error, and therefore power, a lot.
All designs in the package are discussed, and corresponding formula such as these provided, in our technical supplement accompanying this paper and package.


We reduced the number of permutations (to 5000 via `tnum`) to speed up computation.
As `tnum` shrinks, we will get only rough estimates of power, but even these rough estimates can help us determine trends.

The `grid` functions provide easy and direct ways of exploring how power changes as a function of the design parameters.
We note, however, that in order to keep syntax simple, the `grid` methods do not allow different design parameters, including MDES, by outcome.
This is to keep package syntax simpler.
When faced with contexts where it is believed that these parameters do vary, we recommend using average values and then double-checking via the `pump_power` method a small set of potential final designs, to see how the variation across outcomes impacts ones results.


### Exploring the impact of rho

The correlation of test statistics, $\rho$, is a critical parameter for how power will play out across the multiple tests.  For example, if we use Westfall-Young, we know the correlation will improve our individual power, as compared to Bonferroni.
We might not know what will happen to min2 power, however: on one hand, correlated statistics make individual adjustment less severe, and on the other correlation means we succeed or fail all together.
We can explore this relatively easily by letting `rho` vary as so:

```{r, cache=TRUE, fig.height=3, fig.width=7}
grid <- update_grid( p,
            MTP = c( "Bonferroni", "WY-SS" ),
            rho = c( 0, 0.15, 0.3, 0.45, 0.6 ),
            tnum = 500,
            B = 10000 ) 
```

We then plot our results
```{r, cache=TRUE, fig.height=3, fig.width=7}
gridL = filter( grid, MTP != "None" ) %>%
  pivot_longer( cols=c(indiv.mean, min1, min2, complete),
                names_to="definition", values_to="power" ) %>%
  mutate( definition = factor( definition,
                               levels = c("indiv.mean", "min1", "min2", "complete" ) ) )

ggplot( gridL, aes( rho, power, col=MTP ) ) +
  facet_grid( . ~ definition ) +
  geom_line() + geom_point() +
  geom_hline( yintercept =0.80 ) + theme_minimal()
```

First, we see the benefit of the Westfall-Young single-step procedure is minimal in these scenarios as compared to Bonferroni.
Second, the impact on individual adjustment is flat, as anticipated.
Third, across a very broad range of rho, we maintain good min-1 power.
Complete power climbs as correlation increases, and min-2 power is generally unchanged.


### Exploring the impact of null outcomes

We finally explore varying the number of zeros in our outcomes.
For illustration we also expand the number of outcomes to 5.
The tools are the same as before:

```{r, cache=TRUE, fig.width = 7, fig.height = 2.5 }
grid <- update_grid( p,
            numZero = 0:4,
            M = 5 )
```

We then can make a plot as we did above:
```{r, cache=TRUE, echo=FALSE, fig.width = 7, fig.height = 2.5 }
gridL = filter( grid, MTP != "None" ) %>%
  pivot_longer( cols=c(indiv.mean, min1, min2, complete),
                names_to="definition", values_to="power" ) %>%
  mutate( definition = factor( definition,
                               levels = c("indiv.mean", "min1", "min2", "complete" ) ) )

ggplot( gridL, aes( numZero, power ) ) +
  facet_grid( . ~ definition ) +
  geom_line() + geom_point() +
  geom_hline( yintercept =0.80 ) + theme_minimal()

```

With `pump_power()` one can provide an MDES vector with different values for each outcome, including 0s for some outcomes.
For the `grid()` functions, we have a single MDES value for the non-null outcomes, and separately specify how many of the outcomes are 0.
(This single value plus `numZero` parameter also works with `pump_power()` if desired.)


 
## Calculating MDES 

We can use `pum-p` to calculate MDESes as well as power.
To identify the MDES of a given design we use the `pump_mdes` method, which conducts a search for a MDES that achieves a target level of power.
Along with the $p$-value adjustment procedure and the design parameters discussed above, you therefore also need to specify the type (`power.definition`) and desired (`target.power`) power.


Here, for example, we find the MDES for obtaining 80% individual power using the Holm procedure if we had 4 schools per block instead of 3:

```{r MDEScalc, cache=TRUE}
m <- pump_mdes(
           design = "d3.2_m3fc2rc",
           MTP = "Holm",
           target.power = 0.80, power.definition = "D1indiv",
           M = 3, J = 4, K = 21, nbar = 258,
           Tbar = 0.50, alpha = 0.05, numCovar.1 = 5, numCovar.2 = 3,
           R2.1 = 0.1, R2.2 = 0.7, ICC.2 = 0.05, ICC.3 = 0.4, rho = 0.4 )
```

```{r echo = FALSE}
kable(m, digits=2)
```

The `pump_mdes()` method conducts a search, looking for a MDES (assumed shared across all outcomes) to achieve the desired level of power.
The answers it gives are approximate, due to the simulation nature of the power calculations.
To control accuracy, we can specify a tolerance (`tol`) of how close the estimated power needs to be to the desired target along with the number of iterations in the search sequence (via `start.tnum`, `max.tnum`, and `final.tnum`).
The search will stop when the finally estimated power is within `tol` of `target.power`, as estimated by `final.tnum` iterations.
Lower tolerance and higher `tnum` values will give more exact results (and take more computational time).

Changing power definition is straightforward: for example, to identify the MDES for min-1 power (i.e., what effect do we have to assume across all observations such that we will find some significant result with 80% power?), we have a smaller MDES:

```{r MDEScalcmin1, cache=TRUE}
mdes <- update( m, power.definition = "min1" )
```

```{r echo = FALSE}
kable(mdes, digits=2)
```


## Determining necessary sample size

For our design we might want to determine the needed number of students/school, number of schools, or number of blocks needed. The `pump_sample` method will search over any one of these, as requested.

Here we see how many schools are needed to achieve a MDES of 0.05 for complete power (so how many schools are needed to have 80% chance of finding all three outcomes significant, if all outcomes had a true effect size of 0.10).

```{r samplesizecalc, cache=TRUE}
smp <- pump_sample(
  design = "d3.2_m3fc2rc",
  MTP = "Bonferroni",
  typesample = "J",
  target.power = 0.80, power.definition = "complete", tol = 0.01,
  MDES = 0.10, M = 3, nbar = 258, K = 21,
  Tbar = 0.50, alpha = 0.05, numCovar.1 = 5, numCovar.2 = 3,
  R2.1 = 0.1, R2.2 = 0.7, ICC.2 = 0.05, ICC.3 = 0.40, rho = 0.4
)
```

```{r echo = FALSE}
kable(smp)
```

We see we need only a modest increase in the average number of schools per randomization block.
We recommend checking the mdes and sample-size calculators as the estimation error combined with the search can give results a bit off the target in some cases.
Check by running the found design through `pump_power` to see if we recover or originally targeted power (we can use `update()` again for this):

```{r samplesizeverify, cache=TRUE}
p_check <- update( smp, type="power", tnum = 100000 )
```

```{r echo = FALSE}
summary( p_check )
```

We can also look at the power curve to assess how sensitive power is to our level two sample size:

```{r, fig.height = 3.5, fig.width=5}
plot_power_curve( smp )
```

We see that each change in our number of schools per block changes power considerably.
Fractional number of schools per block would correspond to the geometric mean of block size in the case where block sizes varied.





# Guidance for practice 
* Reflections on issues that come up in vignette
  + e.g., how to come up with correlation assumption - what's needed by future researchers so can come to our tool with good assumptions
  + e.g., reinformce importance of looking at ranges when it matters a lot


# Conclusion

We introduce the power under multiplicity project (PUMP) package, which estimates power for multilevel randomized control trials with multiple outcomes.
PUMP allows users to prospectively estimate power, minimum detectable effect size, and sample size requirements for a variety of multilevel RCT designs across different definitions of power and applying different multiple testing procedures.
The functionality of PUMP extends the R package PowerUpR!, an R package that estimates power for many of the same multilevel RCT designs, but only allows for one outcome and does not support multiple testing procedures.

The main function of the PUMP package is to provide easily accessible power estimation procedures so that users can properly account for power when using multiple adjustments.
However, one of the additional strengths of the package is the ease with which a user can explore the impact of different designs, models, and assumptions on power.
Even if a user is only interested in a single outcome, PUMP provides useful functionality for more robust power calculations.
A user can and should try a range of parameter values to determine the sensitivity of the power of their study to different assumptions.

# Appendices
- specifications/derivations for all designs
- how data were generated for simulations
- something about validation maybe? (one example or template?)

# References
