---
title: 'Appendix: Validation of power results'
date: "`r format(Sys.Date(), '%B %d, %Y')`"
header-includes:
   - \usepackage{dsfont}
output:
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document:
    code_folding: hide
    df_print: paged
    fig_caption: yes
    fig_height: 8
    fig_width: 10
    highlight: monochrome
    theme: spacelab
    toc: yes
    toc_depth: 5
    toc_float: yes
---
  
```{r knit.options, echo = FALSE, message = FALSE}
# by default, all code chunks will cache their results
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, echo = FALSE)
library(ggplot2)
library(here)
source(here::here("Validation/Simulations", "misc.R"))
```

# Introduction

This appendix discusses our work to validate that the power estimation methods work as intended.

We compare three different methods of estimating power:
  
* PUMP
* PowerUpR (only comparable for D1individual, unadjusted power)
* Monte Carlo Simulations

We compare point estimates of power from PUMP and PowerUpR for D1 individual, unadjusted power estimates. For all other types of power definitions and adjustments, we are only able to compare PUMP to the estimated power from Monte Carlo simulations. The simulations produce a 95\% confidence interval for power, and we check that the PUMP estimate is within the confidence interval.

## Monte Carlo Simulations

The main work of this validation step was to design monte carlo simulations in order to estimate power.
In order to estimate power by simulation, we follow the following steps.

For iteration $s = 1, \dots S$:
  
1. Generate simulated data according to the assumed data generating process (DGP)
2. Generate simulated treatment assignment
3. Calculate p-value given simulated data, treatment assignment, and assumed model

At the end, a 95\% confidence interval for power is calculated, assuming a conservative standard error estimate of $\sqrt{0.25/S}$.

We also validate MDES and sample size calculations.
For MDES, we choose one default scenario for each design and model, then input the already-calculated D1 individual power and see if the output MDES is the same as the original input MDES.
Similarly, for sample size validation, we input the already-calculated D1 individual power and see if the output sample size (either $J$ or $K$ depending on design) is the same as the original sample size.

## Validation scenarios

We use the following adjustment procedures:
  
  * Bonferroni
* Benjamini Hochberg (BH)
* Holm
* Westfall-Young Single Step (WY-SS)
* Westfall-Young Step Down (WY-SD)

We calculate power under the following definitions:
  
  * Individual power for each outcome (M = 3): D1indiv, D2indiv, D3indiv
* Mean individual power
* Minimum power: min1, min2
* Complete power

We consider the following designs:
  
* Blocked individual randomization, 2 level
  - constant effects (blocked_i1_2c)
  - fixed effects (blocked_i1_2f)
  - random effects (blocked_i1_2r)
* Blocked individual randomization, 3 level
  - random effects (blocked_i1_3r)
* Cluster randomization, 2 level
  - random effects (simple_c2_2r)
* Cluster randomization, 3 level
  - random effects (simple_c3_3r)
* Blocked cluster randomization, 3 level
  - fixed effects (blocked_c2_3f)
  - random effects (blocked_c2_3r)

## Simulation parameters

In order to validate that the method works in a wide range of scenarios, which vary the following parameters.

Parameters that vary:
  
  | Parameter               | Default                | Comparison values |
  ---                       | ---                    | ---
  | school size $\bar{n}$   | 50                     | 75, 100           |
  | $R^2$                   | 0                      | 0.6               |
  | $\rho$                  | 0.5                    | 0.2, 0.8          |
  | ATE (ES) true positives | (0.125, 0.125, 0.125)  | (0.125, 0. 0)     |
  | ICC                     | 0.2                    | 0.7               |
  | $\omega$                | 0.1                    | 0.8               |
  
We do not vary:

* $M = 3$
* $J$ and $K$ are fixed for each scenario
* Scalar grand mean $\Xi_0$
* Correlations between school random effects and impacts $\kappa$
* $\rho$ informs all correlations; we keep the same correlation between covariates, residuals, impacts, random effects for all levels and across all outcomes

TODO: possible things we may want to vary:
  
* Correlations between school random effects and impacts $\kappa$
* Combinations of parameters instead of one at a time
* A realistic set of parameters
* Extreme values, i.e. $\rho = 0.95$
  
\newpage

# Validation results

Below is an example of a graph we use for validation. The red dot is the PUM estimate of power, the green dot is the PowerUpR estimate of power, and the 95% confidence intervals based on the monte carlo simulations are shown in blue. To validate that PUMP produces the expected result, we want to see the red and green points match, and for the red point to be within the blue intervals. The plot shows the results across different types of power and different MTPs. 

This graph in particular is for a blocked cluster design with fixed effects with a particular set of parameters. We repeat this graph for each set of parameters for each design.


```{r}
sim.params.list <- list(
  S = 100                   # Number of samples for Monte Carlo Simulation
  , Q = 10                  # Number of times entire simulation is repeated, so total iterations = S * Q
  , B = NULL                # Number of samples for WestFall-Young. The equivalent is snum in our new method.
  , alpha = 0.05            # Significance level
  , tol = 0.01              # tolerance for MDES and sample  size calculations
  , Tbar = 0.5              # Binomial assignment probability
  , tnum = 10000            # Number of test statistics (samples) for all procedures other than Westfall-Young
  , parallel = TRUE         # parallelize within each monte carlo iteration
  , ncl = 3                 # Number of computer clusters (max on RStudio Server is 16)
  , start.tnum = 2000       # number of iterations for starting to testing mdes and power
  , final.tnum = 100000     # final number of iterations to check power
  , max.steps = 20          # maximum number of iterations for MDES or sample size calculations
  , max.cum.tnum = 10000000 # maximum cumulative tnum for MDES and sample size
  , procs = c("Bonferroni", "BH", "Holm")
  , runSim = TRUE           # If TRUE, we will re-run the simulation. If FALSE, we will pull previous run result.
  , runPump = TRUE          # If TRUE, we will run method from our package. If FALSE, we will pull previous run result.
  , runPowerUp = TRUE       # If TRUE, we will run method from powerup. If FALSE, we will pull previous run result.
)

M <- 3
rho.default <- 0.5
default.rho.matrix <- gen_corr_matrix(M = M, rho.scalar = rho.default)

user.params.list <- list(
  M = 3                                   # number of outcomes
  , J = 60                                # number of schools
  , K = 1                                 # number of districts (for two-level model, set K = 1)
  , nbar = 50                             # number of individuals per school
  , rho.default = rho.default             # default rho value (optional)
  , S.id = NULL                           # N-length vector of indiv school assignments (optional)
  , D.id = NULL                           # N-length vector of indiv district assignments (optional)
  ################################################## grand mean otucome and impact
  , Xi0 = 0                               # scalar grand mean outcome under no treatment
  , ATE_ES = rep(0.125, M)                # minimum detectable effect size      
  ################################################## level 3: districts
  , R2.3 = rep(0.1, M)                    # percent of district variation explained by district covariates
  , rho.V = default.rho.matrix            # MxM correlation matrix of district covariates
  , ICC.3 = rep(0.2, M)                   # district intraclass correlation
  , omega.3 = 0.1                         # ratio of district effect size variability to random effects variability
  , rho.w0 = default.rho.matrix           # MxM matrix of correlations for district random effects
  , rho.w1 = default.rho.matrix           # MxM matrix of correlations for district impacts
  , theta.w = matrix(0, M, M)             # MxM matrix of correlations between district random effects and impacts
  ################################################## level 2: schools
  , R2.2 = rep(0.1, M)                    # percent of school variation explained by school covariates
  , rho.X = default.rho.matrix            # MxM correlation matrix of school covariates
  , ICC.2 = rep(0.2, M)                   # school intraclass correlation	
  , omega.2 = 0.1                         # ratio of school effect size variability to random effects variability
  , rho.u0 = default.rho.matrix           # MxM matrix of correlations for school random effects
  , rho.u1 = default.rho.matrix           # MxM matrix of correlations for school impacts
  , theta.u = matrix(0, M, M)             # MxM matrix of correlations between school random effects and impacts
  ################################################## level 1: individuals
  , R2.1 = rep(0.1, M)                    # percent of indiv variation explained by indiv covariates
  , rho.C = default.rho.matrix            # MxM correlation matrix of individual covariates
  , rho.r = default.rho.matrix            # MxM matrix of correlations for individual residuals 
)

user.params.list[['omega.2']] <- 0
user.params.list[['J']] <- 20
user.params.list[['K']] <- 10
user.params.list[['nbar']] <- 50
sim.params.list[['S']] <- 1000
sim.params.list[['Q']] <- 1
user.params.list[['nbar']] <- 100
```


```{r validate.power, fig.height=8, fig.width=6}
design <- "blocked_c2_3f"
user.params.list[['R2.3']] <- rep(0, M)
user.params.list[['ICC.3']] <- rep(0, M)
params.file.base <- gen_params_file_base(user.params.list, sim.params.list, design)
blocked_c2_3f_nj100_power_plot <- gen.power.results.plot(params.file.base, design)
print(blocked_c2_3f_nj100_power_plot)
```

Next, we validate MDES and sample size calculations. The first column shows the calculated MDES or sample size, the middle column is the power we plugged into the calculation, and the last column shows the MDES or sample size that we are targeting. Thus, ideally we want the first and last columns to match.

```{r}
mdes.file <- find_file(params.file.base, type = 'mdes_D1indiv')
blocked_c2_3f_mdes_D1_results <- readRDS(mdes.file)
pander::pandoc.table(
  blocked_c2_3f_mdes_D1_results,
  style = "grid", split.tables = 100, row.names = FALSE,
  caption = design
)
```

```{r}
sample.file <- find_file(params.file.base, type = 'sample_D1indiv')
blocked_c2_3f_sample_D1_results <- readRDS(sample.file)
pander::pandoc.table(
  blocked_c2_3f_sample_D1_results,
  style = "grid", split.tables = 100, row.names = FALSE,
  caption = design
)
```

# Westfall-Young procedures

The Westfall-Young procedure was validated separately due to its unique complications and computational burden.

First, we wrote a series of careful unit tests ensuring that our code worked as expected for each step of the WY procedure. One of these tests compared results from our procedure to the WY procedure in the multtest package, and found it matched quite well.

Second, we also tested the WY procedures using the same simulation procedure described above.
For constant effect and fixed effect models, the power estimation matches between PUM and the simulations.
However, for random effects models, the two methods diverge in some cases.
We find that the simulated power matches the PUMP-calculated power only when (1) there is a large number of blocks/clusters, and (2) the user has a large number of WY permutations.
Below, we discuss the single-step procedure because it is simpler, although the same concepts hold for the step-down procedure.
Although it is difficult to verify why this discrepancy occurs, our hypothesis is that this behavior occurs due to the combination of the sensitivity of the WY procedure, and the instability of the random effects model.

The goal of the WY procedure is to estimate the adjusted p-value for outcome $i$
$$\tilde{p}_i = Pr\left(\min_{1 \leq j \leq k} P_j \leq p_i \mid H_0^C\right).$$
where $P_j$ is a random variable representing a $p$-value for outcome $j$, and lowercase $p_i$ is the observed realization for outcome $i$.
We assume a set of $k$ tests with corresponding null hypotheses $H_{0i}$ for $i = 1, \cdots, k$.
The complete null hypothesis is the setting where all the null hypotheses are true: $H_0^C = \cap_{i=1}^{k} H_{0i} = \{\text{all } H_i \text{are true}\}$.
In order to estimate this p-value, the p-value is calculated across $B$ permutations
$$\tilde{p}_i = \frac{1}{B}\sum_{b = 1}^{B} \mathds{1}(\min_{1 \leq j \leq k} p_{b,j}^\star \leq p_i).$$

In order for the procedure to be correct, we should be generating the p-values $P_j$ from the the true distribution of p-values under the null hypothesis.
We generally are testing outcomes that are truly significant, so the observed p-values are small.
Thus, looking at these expressions, we are concerned about tail behavior; we are testing whether a small observed p-value is less than the minimum of of the generated p-values of our multiple outcomes.
This combination of factors means that a small discrepancy in the tails between the generated null distribution and the true distribution could have a large impact on the adjusted p-value.

Given that we are relying on tail behavior, this explains why we need both a large number of permutations, and a large number of blocks/clusters.
The large number of permutations is required because we are trying to estimate a rare event.
With a significant outcome, it is rare that the observed p-value will be less than the minimum of p-values generated from the null distribution.
The large number of blocks/clusters is required to accurately estimate the tail behavior for random effects models.
With a small number of blocks/clusters, we may not properly estimate the spread of the distribution, resulting in a poor estimate of the tail behavior.

