@article{RN33088,
   author = {Arnold, B.F., Hogan, D.R., Colford, J.M. Jr. & Hubbard, A.E.},
   title = {Simulation methods to estimate design power: an overview for applied research.},
   journal = {BMC Medical Education Methodology},
   volume = {11, 94},
   pages = {1471-2288},
   DOI = {https://doi.org/10.1186/1471-2288-11-94},
   year = {2011},
   type = {Journal Article}
}

@article{RN33089,
   author = {Bang, H., Jung, S., & George, S.L.},
   title = {Sample size calculation for simulation-based mulitple-testing procedures.},
   journal = {Journal of Biopharmaceutical Statistics},
   volume = {15},
   pages = {957-967},
   year = {2005},
   type = {Journal Article}
}

@article{RN33090,
   author = {Benjamin, Y., & Yekutieli. D. },
   title = {The control of the false discovery rate: A practical and powerful approach to mulitple testing under depedency},
   journal = {Annals of Statistics},
   volume = {29},
   pages = {1165-1188},
   year = {2001},
   type = {Journal Article}
}

@article{RN29695,
   author = {Bloom, Howard S.},
   title = {Minimum Detectable Effects: A Simple Way to Report the Statistical Power of Experimental Designs},
   journal = {Evaluation Review},
   volume = {19},
   number = {547},
   year = {1995},
   type = {Journal Article}
}

@misc{RN27978,
   author = {Bloom, Howard S.},
   title = {The Core Analytics of Randomized Experiments for Social Research},
   publisher = {MDRC},
   year = {2006},
   type = {Government Document}
}

@techreport{RN33091,
   author = {Bretz, F, . Hothorn, T., and  Westfall, P. },
   title = {Multiple comparisons using R.},
   year = {2011},
   type = {Report}
}

@article{RN23882,
   author = {Chen, J., Luo, J., Liu, K., Mehrotra, D.},
   title = {On power and sample size computation for multiple testing procedures},
   journal = {Computational Statistics and Data Analysis},
   volume = {55},
   pages = {110-122},
   year = {2011},
   type = {Journal Article}
}

@unpublished{RN33092,
   author = {Chrstensen, G.S., & Miquel E.},
   title = {Transparancey, Reproducibility, and the Credibility of Economics Research},
   booktitle = {NBER Working Paper #22989},
   year = {2-16},
   type = {Unpublished Work}
}

@article{RN4473,
   author = {Dong, Nianbo and Maynard, Rebecca},
   title = {PowerUP!: A tool for calculating minimum detectable effect sizes and minimum required sample sizes for experimental and quasi-experimental design studies},
   journal = {Journal of Research on Educational Effectiveness},
   volume = {6},
   number = {1},
   pages = {24-67},
   ISSN = {1934-5747},
   year = {2013},
   type = {Journal Article}
}

@article{RN23878,
   author = {Dudoit, S., Shaffer, J.P., Boldrick, J.C.},
   title = {Multiple Hypothesis Testing in Microarray Experiments},
   journal = {Statistical Science},
   volume = {18},
   number = {1},
   pages = {71-103},
   year = {2003},
   type = {Journal Article}
}

@article{RN24280,
   author = {Dunn, Olive Jean},
   title = {Estimation of the Medians for Dependent Variables},
   pages = {192-197},
   abstract = {Joint intervals of bounded confidence are suggested for the medians of a bivariate population with continuous marginal distributions. The two intervals are of the classic type based on sample order statistics.},
   ISSN = {0003-4851},
   DOI = {10.1214/aoms/1177706374},
   url = {http://projecteuclid.org/euclid.aoms/1177706374},
   year = {1959},
   type = {Journal Article}
}

@article{RN24281,
   author = {Dunn, Olive Jean},
   title = {Multiple Comparisons among Means},
   journal = {Journal of the American Statistical Association},
   volume = {56},
   number = {293},
   pages = {52-64},
   ISSN = {0162-1459},
   DOI = {10.1080/01621459.1961.10482090},
   url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1961.10482090},
   year = {1961},
   type = {Journal Article}
}

@book{RN24277,
   author = {Ewens, Warren and Grant, Gregory},
   title = {Statistical Methods in Bioinformatics: An Introduction},
   publisher = {Springer},
   address = {New York},
   series = {Statistics for Biology and Health},
   year = {2005},
   type = {Book}
}

@article{RN33093,
   author = {Ge, Y., D\udoit, S., & Speed, T.P.},
   title = {Resampling-based multiple testing for microarray data analsysis},
   journal = {Test},
   volume = {12},
   pages = {1-77},
   year = {2003},
   type = {Journal Article}
}

@article{RN24272,
   author = {Groppe, David M and Urbach, Thomas P and Kutas, Marta},
   title = {Mass univariate analysis of event‚Äêrelated brain potentials/fields I: A critical tutorial review},
   journal = {Psychophysiology},
   volume = {48},
   number = {12},
   pages = {1711-1725},
   abstract = {Event-related potentials (ERPs) and magnetic fields (ERFs) are typically analyzed via ANOVAs on mean activity in a priori windows. Advances in computing power and statistics have produced an alternative, mass univariate analyses consisting of thousands of statistical tests and powerful corrections for multiple comparisons. Such analyses are most useful when one has little a priori knowledge of effect locations or latencies, and for delineating effect boundaries. Mass univariate analyses complement and, at times, obviate traditional analyses. Here we review this approach as applied to ERP/ERF data and four methods for multiple comparison correction: strong control of the familywise error rate (FWER) via permutation tests, weak control of FWER via cluster-based permutation tests, false discovery rate control, and control of the generalized FWER. We end with recommendations for their use and introduce free MATLAB software for their implementation.},
   keywords = {EEG/ERP; MEG; methods; false discovery rate; permutation test; hypothesis testing},
   ISSN = {1469-8986},
   year = {2011},
   type = {Journal Article}
}

@techreport{RN30153,
   author = {Hedges, Larry V.  and Rhoads, Christopher},
   title = {Statistical Power Analysis in Education Research.},
   institution = {National Center for Special Education Research},
   note = {http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=ED509387},
   abstract = {This paper provides a guide to calculating statistical power for the complex multilevel designs that are used in most field studies in education research. For multilevel evaluation studies in the field of education, it is important to account for the impact of clustering on the standard errors of estimates of treatment effects. Using ideas from survey research, the paper explains how sample design induces random variation in the quantities observed in a randomized experiment, and how this random variation relates to statistical power. The manner in which statistical power depends upon the values of intraclass correlations, sample sizes at the various levels, the standardized average treatment effect (effect size), the multiple correlation between covariates and the outcome at different levels, and the heterogeneity of treatment effects across sampling units is illustrated. Both hierarchical and randomized block designs are considered. The paper demonstrates that statistical power in complex designs involving clustered sampling can be computed simply from standard power tables using the idea of operational effect sizes: effect sizes multiplied by a design effect that depends on features of the complex experimental design. These concepts are applied to provide methods for computing power for each of the research designs most frequently used in education research. Appendices include: (1) Design Effects in Two- or Three-Level Hierarchical Designs With and Without Covariates; (2) Design Effects in Two- or Three-Level Randomized-Block Designs With and Without Covariates; (3) Computing Power in Three-Level Randomized-Block Designs; (4) Multilevel Models Defining Tests for Treatment Effects; and (5) Glossary of Terms. (Contains 5 footnotes and 2 tables.)},
   keywords = {Research Design
Field Studies
Computers
Effect Size
Correlation
Sampling
Computer Software
Multivariate Analysis
Statistical Analysis
Educational Research},
   url = {http://www.eric.ed.gov/ERICWebPortal/detail?accno=ED509387},
   year = {2010},
   type = {Report}
}

@article{RN33094,
   author = {Hochberg, Y},
   title = {A sharper Bonferroni procedure for multiple tests of significance},
   journal = {Blometrika},
   volume = {75},
   pages = {800-802},
   year = {1988},
   type = {Journal Article}
}

@article{RN24282,
   author = {Holm, S.},
   title = {A simple sequentially rejective multiple test procedure.},
   journal = {Scand. J. Statist.},
   volume = {6},
   number = {2},
   pages = {65-70},
   year = {1979},
   type = {Journal Article}
}

@article{RN23880,
   author = {Koch, G. G. and Gansky, M. S.},
   title = {Statistical Considerations for Multiplicity in Confirmatory Protocols},
   journal = {Drug Information Journal},
   volume = {30},
   pages = {523-533},
   year = {1996},
   type = {Journal Article}
}

@inbook{RN33095,
   author = {Maurer, W., and Mellein, B.},
   title = {One new multiple test procedures based on independent p-values and the assessemtn of their powers},
   booktitle = {Multiple Hypotheses Testing},
   editor = {P. bauer, G. Hommel and E. Sonnermann},
   publisher = {Heidelberg, Springer},
   pages = {48-66},
   year = {1988},
   type = {Book Section}
}

@article{RN29875,
   author = {Neyman, Jerzy},
   title = {Statistical Problems in Agricultural Experiments},
   journal = {Journal of the Royal Statistical Society},
   volume = {2},
   pages = {107-180},
   year = {1923},
   type = {Journal Article}
}

@article{Porter2018,
   author = {Porter, Kristin E.},
   title = {Statistical Power in Evaluations That Investigate Effects on Multiple Outcomes: A Guide for Researchers},
   journal = {Journal of Research on Educational Effectiveness},
   volume = {11},
   issue  = {2},
   pages = {267-295},
   year = {2018},
   type = {Journal Article}
}

@article{RN33097,
   author = {Ramsey, P.H.},
   title = {Power differences between pairwise multiple comparisons},
   journal = {Journal of American Statistical Association},
   volume = {75},
   pages = {479-487},
   year = {1978},
   type = {Journal Article}
}

@techreport{RN23884,
   author = {Raudenbush, S.W. and Spybrook, J. and Congdon, R. and Liu, X. and Martinez, A. and Bloom, H. and Hill, C},
   title = {Optimal Design Plus Empirical Evidence (Version 3.0)},
   url = {http://wtgrantfoundation.org/resource/optimal-design-with-empirical-information-od},
   year = {2011},
   type = {Report}
}

@article{RN24128,
   author = {Rubin, Donald B.},
   title = {Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies},
   journal = {Journal of Educational Psychology},
   volume = {66},
   number = {5},
   pages = {688-701},
   abstract = {Randomization should be employed whenever possible but the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases. (Author/BJG)},
   keywords = {Randomization},
   url = {http://www.eric.ed.gov/ERICWebPortal/detail?accno=EJ118470},
   year = {1974},
   type = {Journal Article}
}

@article{RN32385,
   author = {Rubin, Donald B.},
   title = {Causal Inference Through Potential Outcomes and Principal Stratification: Application to Studies with "Censoring" Due to Death},
   journal = {Statistical Science},
   volume = {21},
   number = {3},
   pages = {299-309},
   year = {2006},
   type = {Journal Article}
}

@techreport{RN30147,
   author = {Schochet, Peter Z.},
   title = {Statistical Power for Random Assignment Evaluations of Education Programs},
   institution = {Mathematica Policy Research, Inc., P.O. Box 2393, Princeton, NJ 08543-2393. Tel: 609-799-3535; Fax: 609-799-0005; e-mail: info@mathematica-mpr.com; Web site: http://www.mathematica-mpr.com/publications/.},
   note = {http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=ED489855},
   abstract = {This paper examines issues related to the statistical power of impact estimates for experimental evaluations of education programs. The focus is on "group-based" experimental designs, because many studies of education programs involve random assignment at the group level (for example, at the school or classroom level) rather than at the student level. The clustering of students within groups (units) generates design effects that considerably reduce the precision of the impact estimates, because the outcomes of students within the same schools or classrooms tend to be correlated (that is, are not independent of each other). This, statistical power is a concern for these evaluations. The report is organized into five sections. First, it discusses general issues for a statistical power analysis, including procedures for assessing appropriate precision levels. Second, it discusses reasons that a clustered design reduces the statistical power of impact estimates and provides a simple mathematical formulation of the problem. Third, it presents procedures that can be used to reduce design effects. Fourth, it provides power calculations for impact estimates under various design options and parameter assumptions, and finally, it presents conclusions. Appended is: Values for Factor (.) in Equation (2). (Contains 8 tables.) [This report was submitted by Mathematica Policy Research, Inc. to the Institute of Education Sciences.]},
   url = {http://www.eric.ed.gov/ERICWebPortal/detail?accno=ED489855},
   year = {2005},
   type = {Report}
}

@techreport{RN23748,
   author = {Schochet, Peter Z.},
   title = {Guidelines for Multiple Testing in Impact Evaluations of Educational Interventions. Final Report},
   institution = {Mathematica Policy Research, Inc. P.O. Box 2393, Princeton, NJ 08543-2393. Tel: 609-799-3535; Fax: 609-799-0005; e-mail: info@mathematica-mpr.com; Web site: http://www.mathematica-mpr.com/publications/},
   abstract = {Studies that examine the impacts of education interventions on key student, teacher, and school outcomes typically collect data on large samples and on many outcomes. In analyzing these data, researchers typically conduct multiple hypothesis tests to address key impact evaluation questions. Tests are conducted to assess intervention effects for multiple outcomes, for multiple subgroups of schools or individuals, and sometimes across multiple treatment alternatives. Multiple comparisons issues are not frequently addressed in impact evaluations of educational interventions. The Institute of Education Sciences (IES) at the U.S. Department of Education (ED) contracted with Mathematica Policy Research, Inc. (MPR) to develop guidelines for appropriately handling multiple testing in education research. This report provides a structure to address the multiplicity problem and discusses issues to consider when formulating a testing strategy. Four appendixes are included: (1) Panel Members; (2) Introduction to Multiple Testing; (3) Weighting Options for Constructing Composite Domain Outcomes; and (4) Bayesian Hypothesis Testing Framework (alternative to the classical hypothesis testing framework that is assumed for this report.) (Contains 5 footnotes and 5 tables.)},
   url = {http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=ED502199},
   year = {2008},
   type = {Report}
}

@article{RN23881,
   author = {Senn, Stephen and Bretz, Frank},
   title = {Power and sample size when multiple endpoints are considered},
   journal = {Pharmaceutical Statistics},
   volume = {6},
   pages = {161-170},
   DOI = {10.1002/pst.301},
   year = {2007},
   type = {Journal Article}
}

@article{RN352,
   author = {Shaffer, Juliet Popper},
   title = {Multiple hypothesis testing},
   journal = {Annual Review of Psychology},
   volume = {46},
   number = {1},
   pages = {561-584},
   note = {Used in 2013 SSC report (Bloom and Unterman 2013)},
   keywords = {Methods},
   ISSN = {0066-4308},
   year = {1995},
   type = {Journal Article}
}

@techreport{RN24179,
   author = {Spybrook, Jessica and Bloom, H.S. and Congdon, Richard and Hill, Carolyn J. and Martinez, Andres and Raudenbush, Stephen W.},
   title = {Optimal Design Plus Empirical Evidence: Documentation for the ‚ÄúOptimal Design‚Äù Software Version 3.0.},
   abstract = {The Optimal Design software, developed with support from the National Institute of Mental Health and the William T. Grant Foundation, now contains modules that can assist researchers in planning single level trials, cluster randomized trials, multi-site randomized trials, multi-site-cluster randomized trials, cluster randomized trials with treatment at level three, trials with repeated measures, and cluster randomized trials with repeated measures.},
   url = {http://wtgrantfoundation.org/resource/optimal-design-with-empirical-information-od},
   year = {2011},
   type = {Report}
}

@techreport{RN33098,
   author = {Tukey, J.W.},
   title = {The problem of multiple comparisions},
   institution = {Princeton University},
   year = {1953},
   type = {Report}
}

@book{RN27034,
   author = {U.S. Department of Education, Institute for Education Sciences, What Works Clearinghouse},
   title = {What Works Clearinghouse: Procedures and Standards Handbook (Version 3.0)},
   url = {http://whatworks.ed.gov},
   year = {2013, March},
   type = {Book}
}

@article{RN33100,
   author = {Weiss, M.J., Bloom, H.S., Verbitsky, Savitz, Gupta, N.,  Vigil, A., and Cullinan, D.},
   title = {How much do the effects of eduation and training programs vary across sites? Evidence from exisiting multisite randomized control trials?},
   journal = {Journal of Research on Educational Effectiveness},
   year = {Forthcoming},
   type = {Journal Article}
}

@article{RN33102,
   author = {Westfall, P.H., & Troendle, J.F.},
   title = {Multiple testing with minimal assumptions},
   journal = {Biometrical Journal},
   volume = {50},
   pages = {745-755},
   year = {2008},
   type = {Journal Article}
}

@article{RN33101,
   author = {Westfall, P.H., Tsai, K., Ogenstad, S., Tomoiaga, A., Moseley, S., and Lu, Y.},
   title = {Clincile trials simulation: A statistical approach},
   journal = {Journal of Biopharmaceutical Statistics},
   volume = {18},
   pages = {611-630},
   year = {2008},
   type = {Journal Article}
}

@book{RN28696,
   author = {Westfall, Peter H and Young, S Stanley},
   title = {Resampling-based multiple testing: Examples and methods for p-value adjustment},
   publisher = {John Wiley & Sons},
   volume = {279},
   ISBN = {0471557617},
   year = {1993},
   type = {Book}
}

