---
title: "Summary of package validation results"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document:
    keep_tex: yes
---


```{r knit.options, echo = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, warning = FALSE, message = FALSE, echo = FALSE,
  fig.height = 7
)
opts <- options(knitr.kable.NA = "--")
```

```{r setup}
library(here)
library(kableExtra)
library(magrittr) 
library(PUMP)
library(pander)
library(tibble)
library(tidyr)
# default tables to round to 3 digits
panderOptions('round', 3)
source(here::here("code", "misc.R"))
```

# Introduction

We completed extensive validation checks to ensure our power calculation procedures are correct.
We compared three different methods of estimating power:
  
* `PUMP`: we denote these power estimates $\hat{p}_{pump}$.
* `PowerUpR`: we denote these power estimates $\hat{p}_{pow}$.
* Full Monte Carlo simulations: we denote these power estimates $\hat{p}_{sim}$.

For individual, unadjusted power, we compare `PUMP` to both `PowerUpR` and Monte Carlo simulations.
`PowerUpR` does not support other types of power definitions and MTP adjustments, for all other definitions and adjustments we only compare the estimates from `PUMP` to Monte Carlo simulations.

To produce Monte Carlo estimates, we follow the full simulation approach outlined in detail in Section \ref{sec:est_power}.
For each of $S = 5,000$ iterations, we simulate full dataset, apply the assumed model, and calculate $p$-values for each of the outcomes.
This process gives us a $M \times S$ matrix of $p$-values.
This matrix is then used to calculate empirical estimates of all definitions of power.
To produce 95\% confidence intervals for the power estimates, we construct a confidence interval assuming a conservative standard error estimate of $\sqrt{0.25/S}$.


\subsection{Validation scenarios}

We conducted validation for all designs and models supported by `PowerUpR`.
For each design and model, we ran several scenarios with varying parameter values.
For most scenarios, we vary only one parameter at a time.
Thus, to test varying $\rho$, we set $\rho = 0.2$ with all other parameters being set to the default values, and try another scenario with $\rho = 0.8$ with all other parameters being set to the default values.
Table \ref{tab:val_params} shows the default parameter values, and the other values we try to test out varying that parameter.

\begin{table}
\centering
\begin{tabular}{l l l}
\toprule
Parameter               & Default                & Additional values \\ \midrule
school size $\bar{n}$   & 50                     & 75, 100           \\
$R^2$                   & 0.1                    & 0.6               \\
$\rho$                  & 0.5                    & 0.2, 0.8          \\
MDES                    & (0.125, 0.125, 0.125)  & (0.125, 0, 0)     \\
ICC                     & 0.2                    & 0.7               \\
$\omega$                & 0.1                    & 0.8               \\
\bottomrule
\end{tabular}
\label{tab:val_params}
\caption{Validation parameters}
\end{table}

We do not vary:

* `M = 3` for all designs and models.
* `J` and `K` are fixed for all scenarios for a particular design and model.
* The scalar grand mean of control outcomes is always assumed to be zero.
* The correlations between random effects and random impacts at a particular level are assumed to be zero.

In the `PUMP` package, the user chooses single parameter `rho` that specifies the correlation between test statistics.
In order to generate a full simulated data set, there are a variety of correlation parameters that inform the correlation between outcomes.
In order to ensure that the test statistics have correlation `rho`, we use `rho` as the correlation for all variables in the simulation process.
For example, we may assume that each school has its own intercept and impact for each outcome.
The correlation between the intercept columns for each pair of outcomes is assumed to be `rho`.
The same strategy is used for the following correlations for each level: the correlation between covariates, the correlation between random intercepts (or residuals for level 1), and the correlation between random impacts.
We further discuss the relationship between the correlation between variables and the correlation between test statistics in Section \ref{sec:corr_checker}.


\subsection{Power validation results}

Due to the higher computational burden of Westfall-Young procedures, we first consider validation for the Bonferroni, Holm, and Benjamini-Hochberg procedures.
We consider Westfall-Young procedures in a separate section below.

Figure \ref{fig:validate} is an example of a graph we use for validation.
The green dots are `PUMP` estimate of power, the red dot is the `PowerUpR` estimate of power, and the 95\% confidence intervals based on the Monte Carlo simulations are shown in blue.
To validate that `PUMP` produces the expected result, we want to see the red and green points match, and for the red point to be within the blue intervals.
Figure \ref{fig:validate} shows the results across different types of power and different MTPs.
We repeat this plot for a variety of different parameter values for each design and model.

\begin{figure}[h!]
\centering
  \includegraphics[width=6in]{example_validation_plot.png}
  \caption{Validation plot\label{fig:validate}}
\end{figure}

The full set of these visualizations can be found in the github repository.
Here, we summarize the general results and patterns.

\textbf{Simulated power intervals}.
The Monte Carlo simulations produce a 95\% confidence interval, and we check whether that interval contains the `PUMP` estimates.
In other words, we check if $\hat{p}_{pump}$ is within $\hat{p}_{sim} \pm 1.96 \sqrt{0.25/5000}$.

\textbf{Simulated power point estimates}.
We compare the point estimates from the simulations to `PUMP`.
To do so, we calculate the absolute difference between the power estimates from the simulations and from `PUMP`:
$$ b_{sim} = \lvert \hat{p}_{sim} - \hat{p}_{pump} \rvert.$$

\textbf{PowerUpR power point estimates}.
We compare the point estimates from the `PowerUpR` to `PUMP`.
This comparison is only conducted for individual, unadjusted power.
Similar to the last metric, we calculate the absolute difference between the power estimates from `PowerUpR` and from `PUMP`:
$$ b_{pow} = \lvert \hat{p}_{pow} - \hat{p}_{pump} \rvert.$$

\textbf{Discrepancies between PUMP and simulations.}

In some scenarios, our validation results show some discrepancies.
We discuss the broad categories here.

\textbf{Expected discrepancies}
For all `d2.1_m2fc` and `d2.1_m2ff` designs and models, `PowerUpR` assumes `ICC.2 = 0`, but we do not make that assumption here.
Thus, we expect to see a discrepancy between `PUMP` and `PowerUpR` except for scenarios when we assume `ICC.2 = 0`.
PowerUpR! does allow for a non-zero `ICC.2` for `d2.1_m2fr`.

\textbf{Unexpected discrepancies}
For some of the scenarios, the `PUMP` estimate is slightly outside the range of the simulation intervals.
Our hypothesis is that for these scenarios, the `PUMP` estimate is accurate, but the simulation are producing inaccurate estimates.
All scenarios with unexpected discrepancies occur for random effects models.
In particular, discrepancies occur for the contexts `d2.1_m2fr` (2 scenarios) and `d3.2_m3rr2rc` (6 scenarios).
In general, we find that random effects models are more difficult to fit and less stable than fixed or constant effects models.
For example, for many of the random effects models, the `lme4` package warns that the model has not converged, or has a singular fit.
This instability could result in the simulations achieving poor estimates of power.

In particular, we find that discrepancies occur when random effects models are being fit on models where it is questionably appropriate to apply such a model.
For the context `d2.1_m2fr`, we find that the `PUMP` estimate is slightly outside of the simulation interval for scenarios where either `ICC.2 = 0` or `omega.2 = 0`.
If either of these parameters is zero, there is no treatment effect variation, but the model is attempting to fit random treatment impacts.
Thus, it is not surprising that the simulation models do not perform well in this setting, and we do not necessarily trust power estimates for this scenario.
Similarly, discrepancies occur for the `d3.2_m3rr2rc` context when either `omega.3 = 0` or `ICC.3 = 0`.
In addition, we find three default scenarios for the `d3.2_m3rr2rc` context where the `PUMP` estimate lies outside the simulation interval when neither of these values values is zero.
However, the discrepancy does not occur for all types of power, but only `min1` power, and in one case `D1indiv` power.
For these scenarios, we find that increasing from `K = 10` to `K = 20` reduces or eliminates the discrepancy, lending credence to our hypothesis that the discrepancies are due to poor model fit.   

\textbf{Point estimates}

For each design and model, we calculate a variety of estimates of $b_{sim}$ and $b_{pow}$: we generate an estimate for each MTP and definition of power, across all validation scenario as described above (varying values of design and model parameters).
In the validation github repository, we summarize the mean values of $b_{sim}$ for each design and model, MTP, and definition and power, taking the average across all scenarios.
These full results can be found in the "summary" document in the results folder.
For the sake of brevity, we further summarize here by taking the mean and max values of $b_{sim}$ across all definitions of power and MTPs 
The quantity $b_{pow}$ can only be calculated for individual power with no adjustment, so we summarize it only across validation scenarios.

We find a high concordance between the simulated, `PUMP`, and `PowerUp` estimates.
The summarized results are shown in Table \ref{tab:point}.
In the worst-case scenario, there is a discrepancy of 4\% between the `PUMP` and simulation power estimates, and a discrepancy of 2\% between the `PUMP` and `PowerUp` estimates.
A conservative estimate of the Monte Carlo standard error is $\sqrt{0.25}/5000 = 0.0001$ (where $5000$ is the number of simulation iterations).

\begin{table}[h!]
\centering
\begin{tabular}{lrrr}
\toprule
d\_m & mean.b.sim & max.b.sim & mean.b.pow\\
\midrule
d2.1\_m2fc & 0.006 & 0.008 & 0.002\\
d2.1\_m2ff & 0.007 & 0.010 & 0.002\\
d2.1\_m2fr & 0.016 & 0.024 & 0.013\\
d2.2\_m2rc & 0.007 & 0.011 & 0.006\\
d3.1\_m3rr2rr & 0.008 & 0.013 & 0.006\\
\addlinespace
d3.2\_m3ff2rc & 0.005 & 0.006 & 0.008\\
d3.2\_m3rr2rc & 0.019 & 0.039 & 0.020\\
d3.3\_m3rc2rc & 0.010 & 0.025 & 0.011\\
\bottomrule
\end{tabular}
\caption{Point estimate results for power\label{tab:point}}
\end{table}

\subsection{Westfall-Young procedures}

Above, we have already shown `PUMP` estimates are valid across a wide variety of designs, models, and scenarios.
One remaining piece is to validate that the Westfall-Young procedures are correct.
Because Westfall-Young adjustments are computationally intensive, we consider a smaller number of scenarios for these procedures, but if we validate that the procedure is correct, it should generalize to all designs, models, and scenarios.

First, we wrote a series of careful unit tests ensuring that our code worked as expected for each step of the WY procedure.
One of these tests compared results from our procedure to the WY procedure in the multtest package, and found it matched quite well.

Second, we also tested the WY procedures using the same simulation procedure described above.
For constant effect and fixed effect models, the power estimation matches well between `PUMP` and the simulations.
However, for random effects models, the two methods diverge in some cases.
We find that the simulated power matches the `PUMP`-calculated power only when (1) there is a sufficiently large number of blocks/clusters, and (2) the user has a large number of WY permutations.
Below, we discuss the single-step procedure because it is simpler, although the same concepts hold for the step-down procedure.
Although it is difficult to verify why this discrepancy occurs, our hypothesis is that this behavior occurs due to the combination of the sensitivity of the WY procedure to departures from the true null distribution, and the instability of the random effects model.
Below, we discuss the WY procedure in more detail to better understand why WY adjustment might not produce accurate power estimates in certain settings.

The goal of the WY procedure is to estimate the adjusted $p$-value for outcome $i$
$$\tilde{p}_i = Pr\left(\min_{1 \leq j \leq k} P_j \leq p_i \mid H_0^C\right).$$
where $P_j$ is a random variable representing a $p$-value for outcome $j$, and lowercase $p_i$ is the observed realization for outcome $i$.
We assume a set of $k$ tests with corresponding null hypotheses $H_{0i}$ for $i = 1, \cdots, k$.
The complete null hypothesis is the setting where all the null hypotheses are true: $H_0^C = \cap_{i=1}^{k} H_{0i} = \{\text{all } H_i \text{ are true}\}$.
In order to estimate this $p$-value, the $p$-value is calculated across $B$ permutations
$$\tilde{p}_i = \frac{1}{B}\sum_{b = 1}^{B} 1(\min_{1 \leq j \leq k} p_{b,j}^\star \leq p_i).$$

In order for the procedure to be correct, we need to generate the permuted $p$-values $p_{b,j}^\star$ from the true distribution of $p$-values under the null hypothesis.
However, if our estimation procedure is incorrect, we could be generating $p$-values that do not come from the true null distribution.
For example, with a random effects model that does not converge or has a singular fit, the $p$-values from that model may be inaccurate.

For WY procedures, we are particularly concerned with discrepancies between the true and generated $p$-values that occur in the tails of the distribution.
We generally are testing outcomes that are truly significant, so the observed $p$-values are small.
Then, we are estimating a tail probability: is the observed $p$-value less than the minimum of the generated $p$-values over the multiple outcomes?
At each permutation, we set a binary indicator for whether this occurs, so a small change in $p$-values can flip from an indicator of $0$ to $1$.
Thus, a small discrepancy in the tails between the generated null distribution and the true distribution could have a large impact on the adjusted $p$-value.

Given that we are relying on tail behavior, this explains why we need both a sufficiently large number of permutations, and a sufficiently large number of blocks/clusters.
The large number of permutations is required because we are trying to estimate a rare event.
With a significant outcome, it is rare that the observed $p$-value will be less than the minimum of $p$-values generated from the null distribution.
The large number of blocks/clusters is required in order for the model to generate accurate $p$-value estimates.
With a small number of blocks/clusters, a random effects model has difficulty estimating the spread of the random effects distribution, which could result in inaccurate $p$-values.

*Remark* due to computational burden, for certain designs and models we only tested WY-SS and not WY-SD.
In some cases, we had to reduce the number of simulations, number of permutations, or the number of units in order to run a validation that was not computationally intractable.
We run a single scenario for each design and model, and do not test varying design and model parameters.

\textbf{Discrepancies.} After having adjusted all scenarios to have sufficiently large number of WY permutations and to have enough blocks/clusters, we find that for all scenarios the `PUMP` estimate falls within the simulation interval.
We did not determine a strict rule for what is a sufficient sample size for stable WY estimates.
For the `d3.1_m3rr2rr` context, we found that `J = 5` and `K = 5` produced a poor power estimate, but `J = 10` and `K = 10` produced a good estimate.
For `d3.3_m3rc2rc`, `J = 10` and `K = 10` produced a poor estimate, while `J = 20` and `K = 20` produced a good estimate.

\textbf{Point estimates}. As with other MTPs, we find concordance between the point estimates for simulations and `PUMP`.
The results can be found in Table \ref{tab:wy_point}.

\begin{table}[h!]
\centering
\begin{tabular}{llrr}
\toprule
d\_m & MTP & mean.b.sim & max.b.sim\\
\midrule
d2.1\_m2fc & WY-SD & 0.003 & 0.004\\
d2.1\_m2fc & WY-SS & 0.004 & 0.005\\
d2.1\_m2ff & WY-SD & 0.031 & 0.046\\
d2.1\_m2ff & WY-SS & 0.022 & 0.033\\
d2.1\_m2fr & WY-SD & 0.020 & 0.035\\
\addlinespace
d2.1\_m2fr & WY-SS & 0.025 & 0.051\\
d2.2\_m2rc & WY-SD & 0.006 & 0.021\\
d2.2\_m2rc & WY-SS & 0.011 & 0.023\\
d3.1\_m3rr2rr & WY-SD & 0.026 & 0.037\\
d3.1\_m3rr2rr & WY-SS & 0.040 & 0.056\\
\addlinespace
d3.2\_m3ff2rc & WY-SS & 0.048 & 0.066\\
d3.2\_m3rr2rc & WY-SS & 0.077 & 0.110\\
d3.3\_m3rc2rc & WY-SS & 0.018 & 0.036\\
\bottomrule
\end{tabular}
\caption{Point estimate results for power using WY procedures \label{tab:wy_point}}
\end{table}

\subsection{Validating MDES and Sample size}

For MDES and sample size calculations, we take a different approach.
If the power calculation is correct is across all scenarios, then our MDES and sample size estimates should also be correct across all scenarios as long as the general procedure is correct.
Thus, we choose one default scenario for each design and model and validate MDES and sample size for that single scenario.
First, given a particular MDES, we use `PUMP` to estimate the power.
Then, we reverse the procedure: we use `PUMP` to calculate the MDES given the power estimate we just found.
Finally, we check whether the output MDES is the same as the original input MDES.
We follow the same approach for sample size validation, but repeat the process for each level relevant to that design (`nbar`, `J`, and `K` depending on design).

In Table \ref{tab:mdes}, we provide an illustrative example of validation for a single scenario.
The first column shows the calculated MDES, the middle column is the power we plugged into the calculation, and the last column shows the MDES that we are targeting.
Thus, ideally we want the first and last columns to match.

\begin{table}[h!]
\centering
\begin{tabular}{lrrr}
\toprule
MTP & Adjusted MDES & D1indiv Power & Target MDES\\
\midrule
Bonferroni & 0.122 & 0.447 & 0.125\\
BH & 0.127 & 0.578 & 0.125\\
Holm & 0.125 & 0.540 & 0.125\\
\bottomrule
\end{tabular}
\label{tab:mdes}
\caption{MDES validation}
\end{table}

\textbf{PUMP mdes point estimates}.
We summarize the MDES performance by calculating the absolute difference between the `PUMP` estimate and the target MDES.
$$ b_{mdes} = \lvert \hat{m}_{pump} - \hat{m}_{target} \rvert.$$

For MDES, we only run one scenario per design and model, and we take the mean across the different MTPs.
The results are summarized below.
We find that the `PUMP`-generated MDES is very close to the target MDES.
The results can be found in Table \ref{tab:mdes_point}.

\begin{table}[h!]
\centering
\begin{tabular}{lr}
\toprule
d\_m & $b_{mdes}$\\
\midrule
d2.1\_m2fc & 0.005\\
d2.1\_m2ff & 0.004\\
d2.1\_m2fr & 0.004\\
d2.2\_m2rc & 0.012\\
d3.1\_m3rr2rr & 0.005\\
\addlinespace
d3.2\_m3ff2rc & 0.004\\
d3.2\_m3rr2rc & 0.006\\
d3.3\_m3rc2rc & 0.006\\
\bottomrule
\end{tabular}
\caption{Point estimate results for MDES \label{tab:mdes_point}}
\end{table}

Similarly, we validate our sample size calculations.
Using our found power, we see if `pump_sample()` returns the original sample size.
In Table \ref{tab:ss}, we are targeting a sample size of $J = 20$.

\begin{table}[h!]
\centering
\begin{tabular}{llrr}
\toprule
MTP & Sample.type & Sample.size & D1indiv.power\\
\midrule
Bonferroni & J & 21 & 0.500\\
BH & J & 21 & 0.580\\
Holm & J & 20 & 0.544\\
\bottomrule
\end{tabular}
\label{tab:ss}
\caption{Sample size validation}
\end{table}

It is difficult to summarize sample size performance like we do for power and MDES.
For many designs and models, a range of sample sizes may result in the same power.
Thus, our procedure may produce a valid sample size that is very far off from the input sample size, and summarizing the difference is not illuminating here.
For cases when the sample sizes have a large discrepancy, we plot the power curves to 
check that they are flat, and visually verify that the generated sample size has a similar power
to the input sample size.
These plots can be found in the individual validation documents for each scenario.

```{r}
# extract and summarize results for a particular d_m
get.d_m.power.results <- function(d_m, power.files)
{
  results.files <- grep(d_m, power.files, value = TRUE)
  
  # stack results
  d_m.results <- NULL
  for(file in results.files)
  {
    results <- readRDS(here::here('output', file))

    results.wide <- results %>%
      pivot_wider(names_from = c(method, value.type), values_from = value)
    
    d_m.results <- rbind(d_m.results, results.wide)
  }

  row.names(d_m.results) <- NULL
  d_m.results %<>% mutate(
    cover =
      Sim_ci_lower <= PUMP_adjusted_power &
      Sim_ci_upper >= PUMP_adjusted_power,
    bias.sim = abs(Sim_adjusted_power - PUMP_adjusted_power),
    bias.pow = abs(PowerUp_adjusted_power - PUMP_adjusted_power)
  )
  d_m.results$d_m <- d_m
  
  # append information about whether we expect results to agree
  d_m.results$agree <- TRUE
  if(d_m %in% c('d2.1_m2fc', 'd2.1_m2ff'))
  {
    d_m.results$agree[d_m.results$ICC.2 > 0] <- FALSE
  }

  return(d_m.results)
}
```


```{r}
# find files
all.files <- list.files(here::here('output'))
# restrict to relevant files
power.files <- grep('power', all.files, value = TRUE)
# subset into WY and not WY
main.power.files <- grep('__B', power.files, value = TRUE)
wy.power.files <- power.files[!(power.files %in% main.power.files)]

d_m.list <- c(
  "d2.1_m2fc", "d2.1_m2ff", "d2.1_m2fr",
  "d2.2_m2rc", "d3.1_m3rr2rr", "d3.2_m3ff2rc", "d3.2_m3rr2rc", "d3.3_m3rc2rc"
)

all.power.results <- NULL
for(d_m in d_m.list)
{
  d_m.results <- get.d_m.power.results(d_m, main.power.files)
  all.power.results <- rbind(all.power.results, d_m.results)
}
```

```{r}
# summarize coverage
cover.results <- ddply(
  all.power.results[all.power.results$agree,], c('d_m', 'MTP', 'power_type'),
  summarise,
  cover = mean(cover)
)
```

```{r}
# summarize bias
bias.results <- ddply(
  all.power.results[!is.na(all.power.results$cover) & all.power.results$cover,],
  c('d_m', 'MTP', 'power_type'),
  summarise,
  mean.b.sim = mean(bias.sim),
  mean.b.pow = mean(bias.pow[agree == TRUE], na.rm = TRUE)
)
# clean up for table printing
bias.results$mean.b.pow[is.nan(bias.results$mean.b.pow)] = NA
```

\newpage

# Summary of validation coverage results

```{r, results = 'asis'}
for(scenario in d_m.list)
{
  table.data <- dplyr::filter(cover.results, d_m == scenario)
  table.data <- table.data  %>%
    dplyr::arrange(MTP, power_type)
  
  print(kable(table.data, booktabs = TRUE, row.names = FALSE, digits = 3,
              linesep = c("", "", "", "", "", "", "\\addlinespace")))
}
```

\clearpage
\newpage

# Coverage discrepancies

We summarize below the scenarios where the simulation intervals do not cover the PUMP value.
For brevity, we only display results for Bonferroni adjustments.

First, the scenarios themselves.

```{r}
# extract out problem rows
problems <- all.power.results[
  !is.na(all.power.results$cover) &
  all.power.results$cover != 1 &
  all.power.results$agree,
]

problem.scenarios <- unique(problems[,
  c('d_m', 'numZero', 'J', 'K', 'nbar', 'omega.2', 'omega.3',
    'R2.1', 'R2.2', 'R2.3', 'ICC.2', 'ICC.3', 'rho')])
kable(problem.scenarios, booktabs = TRUE, row.names = FALSE, digits = 3)
```

Next, the power estimates from these scenarios.

```{r, results = 'asis'}
problems <- problems %>%
  dplyr::select(d_m, MTP, power_type, omega.2, omega.3, ICC.2, ICC.3, R2.2,
         PUMP_adjusted_power, Sim_adjusted_power,
         Sim_ci_lower, Sim_ci_upper) %>%
  dplyr::rename(
         type = power_type,
         pump = PUMP_adjusted_power,
         sim = Sim_adjusted_power,
         low = Sim_ci_lower,
         up = Sim_ci_upper)

problem.table <- data.frame(problems)

for(scenario in d_m.list)
{
  problem.table.data <- dplyr::filter(problem.table, d_m == scenario, MTP == 'BF')
  
  if(nrow(problem.table.data) > 0)
  {
    print(kable(problem.table.data, booktabs = TRUE, row.names = FALSE, digits = 2))
    cat('\n\n\n')
  }
}
```

```{r}
# sim.params.list <- list(
#   S = 5000                    # Number of samples for Monte Carlo Simulation
#   , B = NULL                 # Number of samples for WestFall-Young. The equivalent is snum in our new method.
#   , alpha = 0.05             # Significance level
#   , tol = 0.01               # tolerance for MDES and sample  size calculations
#   , Tbar = 0.5               # Binomial assignment probability
#   , tnum = 10000             # Number of test statistics (samples) for all procedures other than Westfall-Young
#   , parallel = TRUE          # parallelize within each monte carlo iteration
#   , ncl = 8                  # Number of computer clusters
#   , start.tnum = 200         # number of iterations for starting to testing mdes and power
#   , final.tnum = 100000      # final number of iterations to check power
#   , max.steps = 20           # maximum number of iterations for MDES or sample size calculations
#   , max.cum.tnum = 10000000  # maximum cumulative tnum for MDES and sample size
#   , MTP = c("BF", "BH", "HO") # Multiple testing procedures
#   , runSim = FALSE       # If TRUE, we will re-run the simulation. If FALSE, we will pull previous run result.
#   , runPump = TRUE    # If TRUE, we will run method from our package. If FALSE, we will pull previous run result.
#   , runPowerUp = TRUE    # If TRUE, we will run method from powerup. If FALSE, we will pull previous run result.
# )
```


```{r}
# M <- 3
# model.params.list <- list(
#   M = 3                                   # number of outcomes
#   , J = 30                                # number of schools
#   , K = 10                                # number of districts (for two-level model, set K = 1)
#   , nbar = 50                             # number of individuals per school
#   , rho.default = 0.5                     # default rho value
#   , MDES = rep(0.125, M)                  # minimum detectable effect size      
#   , ICC.3 = rep(0.7, M)                   # district intraclass correlation
#   , omega.3 = rep(0.1, M)                 # ratio of district effect size variability to random effects variability
#   , numCovar.2 = 1                        # number of school covariates
#   , R2.2 = rep(0.1, M)                    # percent of school variation explained by school covariates
#   , ICC.2 = rep(0.2, M)                   # school intraclass correlation	
#   , numCovar.1 = 1                        # number of individual covariates
#   , R2.1 = rep(0.1, M)                    # percent of indiv variation explained by indiv covariates
# )
# d_m <- "d3.2_m3rr2rc"
# plot1 <- gen.power.results.plot(gen_params_file_base(model.params.list, sim.params.list, d_m), d_m)
# print(plot1)
# 
# model.params.list$ICC.3 <- rep(0.2, M)
# model.params.list$R2.2 <- rep(0.6, M)
# plot2 <- gen.power.results.plot(gen_params_file_base(model.params.list, sim.params.list, d_m), d_m)
# print(plot2)
# 
# model.params.list$ICC.3 <- rep(0.2, M)
# model.params.list$R2.2 <- rep(0.6, M)
# model.params.list$J <- 50
# plot3 <- gen.power.results.plot(gen_params_file_base(model.params.list, sim.params.list, d_m), d_m)
# print(plot3)
```


\clearpage
\newpage


# Summary of validation "bias" results

```{r, results = 'asis'}
for(scenario in d_m.list)
{
  table.data <- dplyr::filter(bias.results, d_m == scenario)
  table.data <- table.data  %>%
    dplyr::arrange(MTP, power_type)
  
  print(kable(table.data, booktabs = TRUE, row.names = FALSE, digits = 3,
              linesep = c("", "", "", "", "", "", "\\addlinespace")))
}
```

\newpage

# WY Summary

```{r}
# subset down to WY results which we know are "valid" to get accurate estimates
# e.g. if B is too small, or J or K are too small and don't validate,
# exclude those
wy.power.files <- c(
   "d2.1_m2fc_2000_S_3_M_1000_B_005005005_MDES_60_J_1_K_30_nbar_05_rho__omega2__omega3_010101_R21__R22__R23_020202_ICC2__ICC3_comparison_power_results.RDS",                               
  "d2.1_m2ff_2000_S_3_M_1000_B_005005005_MDES_60_J_1_K_30_nbar_05_rho_010101_omega2__omega3_010101_R21__R22__R23_020202_ICC2__ICC3_comparison_power_results.RDS",                       
  "d2.1_m2fr_2000_S_3_M_1000_B_005005005_MDES_60_J_1_K_30_nbar_05_rho_010101_omega2__omega3_010101_R21__R22__R23_020202_ICC2__ICC3_comparison_power_results.RDS",                    
  "d2.2_m2rc_2000_S_3_M_1000_B_025025025_MDES_60_J_1_K_50_nbar_05_rho__omega2__omega3_010101_R21_010101_R22__R23_010101_ICC2__ICC3_comparison_power_results.RDS",                     
  "d3.1_m3rr2rr_600_S_3_M_2000_B_012501250125_MDES_10_J_10_K_100_nbar_05_rho_005005005_omega2_005005005_omega3_010101_R21__R22__R23_010101_ICC2_010101_ICC3_comparison_power_results.RDS",
  "d3.2_m3ff2rc_1000_S_3_M_2000_B_025025025_MDES_5_J_10_K_50_nbar_05_rho__omega2__omega3_040404_R21_040404_R22_040404_R23_010101_ICC2_010101_ICC3_comparison_power_results.RDS",    
  "d3.2_m3rr2rc_600_S_3_M_3000_B_012501250125_MDES_20_J_20_K_75_nbar_05_rho__omega2_005005005_omega3_020202_R21_020202_R22_020202_R23_020202_ICC2_020202_ICC3_comparison_power_results.RDS",
  "d3.3_m3rc2rc_600_S_3_M_3000_B_030303_MDES_20_J_20_K_100_nbar_05_rho__omega2__omega3_040404_R21_040404_R22_040404_R23_005005005_ICC2_005005005_ICC3_comparison_power_results.RDS"     
)
```


```{r}
wy.power.results <- NULL
for(d_m in d_m.list)
{
  d_m.results <- get.d_m.power.results(d_m, wy.power.files)
  wy.power.results <- rbind(wy.power.results, d_m.results)
}
wy.power.results <- wy.power.results[wy.power.results$MTP %in% c('WY-SS', 'WY-SD'),]
```


```{r}
wy.cover.results <- ddply(
  wy.power.results, c('d_m', 'MTP', 'power_type'),
  summarise,
  cover = mean(cover)
)
```


```{r}
# summarize bias
wy.bias.results <- ddply(
  wy.power.results[!is.na(wy.power.results$cover) & wy.power.results$cover,],
  c('d_m', 'MTP', 'power_type'),
  summarise,
  mean.b.sim = mean(bias.sim)
)
```

```{r}
wy.bias.results.summary <- ddply(
  wy.bias.results, c('d_m', 'MTP'),
  summarise,
  avg.b.sim = mean(mean.b.sim),
  max.b.sim = max(mean.b.sim)
)
colnames(wy.bias.results.summary) <- c('d_m', 'MTP', 'mean.b.sim', 'max.b.sim')
kable(wy.bias.results.summary, booktabs = TRUE, row.names = FALSE, digits = 3)
# kable(wy.bias.results.summary, booktabs = TRUE, row.names = FALSE, digits = 3,
# format = 'latex')
```

# MDES summary


```{r}
# find files
all.files <- list.files(here::here('output'))
# restrict to relevant files
mdes.files <- grep('mdes', all.files, value = TRUE)
mdes.files <- grep('5000_S', mdes.files, value = TRUE)
```

```{r}
# extract and summarize results for a particular d_m
get.d_m.mdes.results <- function(d_m,  mdes.files)
{
  results.files <- grep(d_m, mdes.files, value = TRUE)
  # remove WY files
  mdes.file <- grep('__B', results.files, value = TRUE)
  
  results <- readRDS(here::here('output', mdes.file))

  results$abs.rel.bias <- abs(results$`Adjusted MDES` - results$`Target MDES`) /  results$`Target MDES`

  return(results)
}
```


```{r}
all.mdes.results <- NULL

for(d_m in d_m.list)
{
  d_m.results <- get.d_m.mdes.results(d_m, mdes.files)
  all.mdes.results <- rbind(all.mdes.results, d_m.results)
}
```

```{r}
kable(all.mdes.results[,1:5], booktabs = TRUE, row.names = FALSE, digits = 3)
```


\newpage

# Collapsed Summaries

```{r}
bias.results.summary <- ddply(
  bias.results, 'd_m',
  summarise,
  avg.b.sim = mean(mean.b.sim),
  max.b.sim = max(mean.b.sim),
  mean.b.pow = mean(mean.b.pow, na.rm = TRUE)
)
colnames(bias.results.summary) <- c('d_m', 'mean.b.sim', 'max.b.sim', 'mean.b.pow')
kable(bias.results.summary, booktabs = TRUE, row.names = FALSE, digits = 3)
# kable(bias.results.summary, booktabs = TRUE, row.names = FALSE, digits = 3,
# format = 'latex')
```

```{r}
mdes.results.summary <- ddply(
  all.mdes.results, 'd_m',
  summarise,
  mean.b.mdes = mean(abs.rel.bias, na.rm = TRUE)
)
kable(mdes.results.summary, booktabs = TRUE, row.names = FALSE, digits = 3)
# kable(mdes.results.summary, format = 'latex', booktabs = TRUE, row.names = FALSE, digits = 3)
```


