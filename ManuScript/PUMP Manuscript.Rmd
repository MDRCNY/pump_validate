---
title: "PUMP manuscript draft"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---
Power Under Multiplicity Project (PUMP): An R package for estimating statistical power, minimum detectable effect sizes (MDES's) and sample sizes when adjusting for multiple hypothesis tests
Draft outline

# Introduction
* Available software for estimating statistical power, MDES and sample sizes for RCT's in education research
- world of modest number of outcomes (relative to some science applications)
* Brief summary of the multiple testing problem

TESTING!


This paper focuses on the frequentist framework of hypothesis testing, as it is currently the prevailing framework in education and social policy research. Under this framework, the treatment and control groups in an RCT are considered random samples from a defined population (assumed to be the same across all blocks under the assumed design). Following the Rubin-Neyman counterfactual framework (Neyman, 1923; Rubin, 1974, 2006), $Y0_i(m)$ is the $m^{th}$ of $M$ outcomesfor individual $i$ when not exposed to the treatment, and $Y1_i(m)$ is the $m^{th}$ of $M$ outcomes for individual $i$ when exposed to treatment. Then the population average treatment on the th m outcome, given by

\begin{equation}
\psi(m) = E(Y1_i(m)) - E(Y0_i(m)),
\end{equation}

is considered to be fixed. Researchers often express the average treatment effect in standard deviation units --- as an effect size. The effect size parameter for the $m^{th}$ outcome is given by

\begin{equation}
ES(m) = \frac{\psi(m)}{\sigma_Y(m)},
\end{equation}

where $\sigma_Y(m)$ is the standard deviation of the $m^th$ outcome.^[It is assumed here that the standard deviation is the same in both counterfactual settings.]

In the frequentist framework, one typically tests a null hypothesis of no effect, $H0(m): ES(m)=0$, against an alternative hypothesis of $H1(m): ES(m)\=0 for a two-sided tests or $H1(m): ES(m)>0 or H1(m): ES(m)<0 for a one-sided test. For the purposes of computing
power researchers specify an alternative hypothesis of at least a particular effect size (ES). In the above example, the researchers specified an ES of 0.125 A significance test, such as a two-sided or one-sided t-test, is then conducted, and one obtains a test statistic given by

\begin{equation}
t(m) = \frac{ES(m)}{SE(ES(m))},
\end{equation}

from which a raw p-value is computed. Here, the term ârawâ is used to distinguish this p-value from a p-value that has been adjusted for multiple hypothesis tests, as discussed below. The raw p-value is the probability of a test statistic being at least as extreme as the one observed, given that the null hypothesis is true. For a two-sided test, which is the focus of this paper going forward, the raw pvalue for the th m test is $p(m)=2*Pr{T(m)>=|t(m)|}$.^[For a one-sided test, depending on the direction of our alternative hypothesis, the raw p-value for the $m^{th}$ test is computed as $p(m)=Pr{T(m)â¥t(m)}$ or $p(m)=Pr{T(m)â¤t(m)}$.] This expression means we use our knowledge of the sampling distribution of the _t-_statistic, and we identify where our observed test statistic falls in that distribution when it is centered around zero.

When testing a _single_ hypothesis under this framework (such effects are being assessed on just one outcome, so that $M=1$), researchers typically specify an acceptable maximum probability of making a Type I error, $\alpha$. A Type I error is the probability of erroneously rejecting the null hypothesis when it is true. The quantity $\alpha$ is also referred to as the significance level. If $\alpha=0.05$, then the null hypothesis is rejected if the p-value is less than 0.05, and it is concluded that the intervention had an effect because there is less than a 5% chance that this finding is a false positive.

When one tests _multiple_ hypotheses under this framework (such that $M>1$) and one conducts a separate test for each of the hypotheses with $\alpha=0.05$ there is a _greater_ than 5% chance of a false positive finding in the study. If the multiple tests are independent, the probability that at least one of the null hypothesis tests will be erroneously rejected is 1-Pr(none of the null hypotheses will be erroneously rejected) $= 1-(1-\alpha)^m$. Therefore, in the above motivating example in which the researchers are estimating effects on three outcomes, if these outcomes are assumed independent, the probability of at least one false positive finding is 14%. If the researchers were instead estimating effects on five independent outcomes, the probability of at least one false positive finding is 23%. This Type I error inflation for independent outcomes demonstrates the crux of the multiple testing problem. In practice, however, the multiple outcomes are at least somewhat correlated, which makes the test statistics correlated and reduces the extent of Type I error inflation. Nonetheless, any error inflation can still make it problematic to draw reliable conclusions about the existence of effects. As introduced above, to counteract the multiple testing problem, MTPs adjust p-values upward.^[Alternatively, MTPs can decrease the critical values for rejecting hypothesis tests. For ease of presenta-tion, this paper focuses only on the approach of increasing p-values.] The sections that follow will describe how the MTPs do so.

Recall that the power of an individual hypothesis test is the probability of rejecting a false null hypothesis of at least a specified size. If raw p-values are adjusted upward, one is less likely to reject the null hypotheses that are true (meaning there is truly no effect of at least a specified size), which reduces the probability of Type I errors, or false positive findings. Reducing this probability is the goal of MTPs. But if raw p-values are adjusted upward, one is also less likely to reject the null hypotheses that are false (meaning there truly is an effect of at least a specified size). There-fore, all MTPs reduce individual power (the power of separate hypothesis tests for each outcome) compared with the situation when no multiplicity adjustments are made or the situation when there is only one hypothesis test.

MTPs also reduce all other definitions of power compared with the situation when no mul-tiplicity adjustments are made â but not necessarily compared with the situation when there is only one hypothesis test. For example, 1-minimal power, the probability of detecting effects (of at least a specified size) on at least one outcome â after adjusting for multiplicity â is typically greater than the probability of detecting an effect of the same size on a single outcome. This increase may or may not occur with other definitions of power (e.g., the probability of detecting a third, half, or all false null hypotheses).


* Common MTPs in education research and their impact on power, MDES's and sample sizes

The MTPs that are the focus of this paper fall into two different classes. The first class reframes Type I error as a rate across the entire set or âfamilyâ of multiple hypothesis tests. This rate is called the familywise error rate (FWER; Tukey, 1953). It is typically set to the same value as the probability of a Type I error for a single test, or to Î±. MTPs that control the FWER at 5% adjust p-values in a way that ensures that the probability of at least one Type I error across the entire set of hypothesis tests is no more than 5%. The MTPs introduced by BonferÂ¬roni (Dunn, 1959, 1961), Holm (1979), and Westfall and Young (1993) control the FWER.

The second class of MTPs takes an entirely different approach to the multiple testing problem. MTPs in this class control the false discovery rate (FDR). Introduced by Benjamini and Hochberg (1995), the FDR is the expected proportion of all rejected hypotheses that are erroneously rejected.

The two-by-two representation in Table 1 is often found in articles on multiple hypothesis testing. It helps to illustrate the difference between FWER and FDR. Let $M$ be the total number of tests. Therefore, we have M unobserved truths: whether or not the null hypotheses are true or false. We also have M observed decisions: whether or not the null hypotheses were rejected, because the p-values were less than Î±. In Table 1, A,B,C, and D are four possible scenarios: the numbers of true or false hypotheses not rejected or rejected. M0 and M1 are the unobservable numbers of true null and false null hypotheses. R is the number of null hypotheses that were rejected, and M-R is the number of null hypotheses that were not rejected.

In Table 1, $B$ is the number of erroneously rejected null hypotheses, or the number of false positive findings. Therefore, the FWER is equivalent to $Pr(B>0)$, the probability of at least one false positive finding. Recall the examples above about Type I error inflation when testing for effects on independent outcomes in the case that $\alpha$ is set to 0.05 and no MTPs are applied. The Type I error was almost 10% when testing effects on two independent outcomes and 23% when testing effects on five independent outcomes. These Type I error rates both correspond to the FWER. The goal of MTPs that control the FWER is to bring these percentages back down to 5%.

Also in Table 1, the FDR is equal to $E(\frac{B}{R})$ but is defined to be 0 when $R=0$, or when no hypotheses are rejected. As is frequently noted in the literature (e.g., Shaffer, 1995; Schochet, 2008), the FWER and FDR have different objectives. Control of the FWER protects researchers from any spurious findings and so may be preferred when even a single false positive could lead to the wrong conclusion about the effectiveness of an intervention. On the other hand, the FDR is more lenient with false positives. Researchers may be willing to accept a few false positives, B, when the total number of rejected hypotheses, R, is large. Note that under the complete null hypothesis that all M null hypotheses are null, the FDR is equal to the FWER, because when referring back to Table 1 we have FWER=P(R>0)=E(B/R)=FDR. However, if any effects truly exist, then FWERâ¥FDR. As a result, in the case where there is at least one false null hypothesis (at least one true effect at least as large as a specified effect size), an MTP that controls the FDR at 5% will have a Type I error rate that is greater than 5%.

Note that MTPs may provide either weak or strong control of the error rate they target. An MTP provides weak control of the FWER or the FDR at level $\alpha$ if the control can only be guaranteed when all nulls are true, or when the effects on all outcomes are zero. An MTP provides strong control of the FWER or FDR at level Î± if the control is guaranteed when some null hypotheses are true and some are false, or when there may be effects on at least some outcomes. Of course, strong control is preferred. 


* Important contributions of the PUMP package

# Estimating Power, MDES's and Sample Sizes in Studies of Impacts on Multiple Outcomes 
* Approach for estimating statistical power
  + General strategy
  + Specifications for each RCT design (in appendix)
* Approach for estimating MDES's and sample size
* Modeling empirical findings for faster estimation
* Validation (how much/what should we present?)
  + paragraph on what we did 
  + consider options for making code available

# Software
* Map of PUMP functions (functions/naming convention for each design)
* Inputs 
* Outputs
 + for shiny, we will create plots - better to make them part of the package?
* To what extent should we cover Shiny app in the paper? 

# Example(s)
* maybe one blocked individual RCT and one cluster RCT?
* maybe one is scenario for designing study and one is scenario for post-hoc power analysis?
 + post hoc might be best place to start since simplest
 + then more about design considerations after making software clear

# Guidance for practice (or after software section below?)
* Considerations for study design - limted to implications from multiple testting plus short lit review of key issues related to design issues 
* Considerations for specifying assumptions (or do we go through this when discussing inputs for R functions)
  + how to come up with correlation assumption
  + importance of looking at ranges when it matters a lot

# Conclusion and Next Steps






* Common MTPs in education research and their impact on power, MDES's and sample sizes
* Important contributions of the PUMP package

# Estimating Power, MDES's and Sample Sizes in Studies of Impacts on Multiple Outcomes 
* Approach for estimating statistical power
  + General strategy
  + Specifications for each RCT design (in appendix)
* Approach for estimating MDES's and sample size
* Modeling empirical findings for faster estimation
* Validation (how much/what should we present?)
  + paragraph on what we did 
  + consider options for making code available

# Software
* Map of PUMP functions (functions/naming convention for each design)
* Inputs 
* Outputs
 + for shiny, we will create plots - better to make them part of the package?
* To what extent should we cover Shiny app in the paper? 

# Example(s)
* maybe one blocked individual RCT and one cluster RCT?
* maybe one is scenario for designing study and one is scenario for post-hoc power analysis?
 + post hoc might be best place to start since simplest
 + then more about design considerations after making software clear

# Guidance for practice (or after software section below?)
* Considerations for study design - limted to implications from multiple testting plus short lit review of key issues related to design issues 
* Considerations for specifying assumptions (or do we go through this when discussing inputs for R functions)
  + how to come up with correlation assumption
  + importance of looking at ranges when it matters a lot

# Conclusion and Next Steps




